[{"content":"Profiling the CPU allows us to analyze the program\u0026rsquo;s performance, identify bottlenecks, and optimize its efficiency.\nHave you ever wondered what happens behind the scenes when you run a program and how to account for CPU time for the actual program functions? And even more, how to write such a tool to profile the program?\nEven though great open-source projects provide continuous profiling with vast support for compiled, JITed, and interpreted, languages, with or without debug info, with or without frame pointers, etc., don\u0026rsquo;t be discouraged!\nWriting your own can be a fantastic learning experience. Building your own profiler offers a unique challenge and the satisfaction of unlocking powerful performance analysis tools.\nThis blog series will embark on a journey to give you the basics for writing a program profiler.\nIn this first episode, we\u0026rsquo;ll establish the foundation by exploring the program execution environment. We\u0026rsquo;ll dig into how the CPU executes a program and keeps track of the execution flow. Finally, we\u0026rsquo;ll discover how this tracking data is stored and becomes the key to unlocking the profiling primitives.\nIntroduction We know that the CPU executes the programs and that the program\u0026rsquo;s binary instructions are stored in a volatile memory which is the random access memory.\nAs RAM locations are byte-addressable the CPU needs a way to keep track of the addresses in order to retrieve the data from it, which is in our case CPU instructions that are then executed.\nThe CPU uses small built-in memory areas called registers to hold data retrieved from the main memory. Registers come in two types: general-purpose and special-purpose. Special-purpose registers include pointer registers, which are designed specifically to store pointers, which means, they store the memory address\u0026rsquo;s value.\nThere are other types of registers but they\u0026rsquo;re out of scope for this walkthrough.\nThe first part will go through the main pointer registers, which are commonly implemented by the predominant architectures (x86, ARM, MIPS, PowerPC as far as I know). So, please consider that these specifics may differ depending on the architecture.\nThe good, the bad and the ugly pointer registers The program counter The program counter (PC), often also called instruction pointer (IP) in x86 architectures, is a register that points to code, that is, the instruction that will be executed next. The instruction data will be fetched, will be stored in the instruction register (IR), and executed during the instruction cycle. You can follow a diagram of a simplified instruction cycle in the picture below:\n The CPU control unit (CU) read the value of the PC It sends it to the CPU Memory Unit (MU) The MU reads the instruction code from the memory at the address pointed to by the PC The MU stores the opcode to the IR The MU reads the opcode The MU sends the opcode to the CU The CU instructs the Register File (RF) to read operands - if available from registers, I\u0026rsquo;m simplifying - from general purpose registers (GPR) The RF reads operands from GPRs The CU sends them to the Arithmetic Logic Unit (ALU), which calculates and stores the result in its temporary memory The CU requests the ALU to perform the arithmetic and logic operations The RF reads the result from the ALU The RF stores the AL result in GPRs  For example, considering a CALL instruction, this could be the flow considering the PC, the IR and the mainly involved general purpose registers to store the operands:\nDepending on the instruction set, the PC will be increased instruction by instruction by the instruction size (e.g. 8 bytes on 64 but Instruction Set Architectures).\nIn an executable file, the machine code to be executed by the CPU is usually stored in a dedicated section, depending on the executable format. For example, in ELF (Executable and Linkable Format) the machine code is organized in the .text section.\nThe stack pointer On the other side, the stack pointer (SP) and base pointer (BP) point to the stack, which contains data about the program being executed.\nWhile a detailed explanation of the stack is beyond the scope of this blog, here\u0026rsquo;s a basic idea: it\u0026rsquo;s a special area of memory that the CPU uses to manage data related to the program\u0026rsquo;s functions (subroutines) as they are called and executed, pushing it to it in a LIFO method. We\u0026rsquo;ll see later on in more detail.\nData and code are organized in specific regions inside the process address space. It\u0026rsquo;s constantly updated by the CPU on push and pop operations on the stack. The stack pointer is usually set by the OS during the load to point to the top of the stack memory region.\nAs the stack grows whenever the CPU adds new data while executing the program\u0026rsquo;s instructions, the stack pointer decrements and is always at the lowest position in the stack.\n Remember: the stack grows from the highest address to the lowest address:\n So, when a new variable of 4 bytes is declared, the stack pointer will be decreased by 4 bytes too.\nFor instance, considering a C function that declare a local variable:\nvoid myFunction() { int localVar = 10; // Local variable declaration  // Use localVar here } the simplified resulting machine code could be something like the following:\n; Allocate space for local variables (assuming 4 bytes for integer) sub rsp, 4 ; Subtract 4 from stack pointer (SP) to reserve space ; Move value 10 (in binary) to localVar's memory location mov dword ptr [rsp], 10 ; Move 10 (dword = 4 bytes) to memory pointed to by SP (stack top) ; ... ; Function cleanup (potential instruction to restore stack space) add rsp, 4 ; Add 4 back to stack pointer to deallocate local variable space  Clarification about the register names\nYou\u0026rsquo;ll find different names for these pointer registers depending on the architectures. For example for x86:\n On 16-bit architecture are usually called sp, bp, and ip. Instead on 32-bit esp, ebp, and eip. Finally, on 64-bit they\u0026rsquo;re usually called rsp, rbp, and rip.   Specifically, a stack pointer (SP) points to the first free and unused address on the stack. It can reserve more space on the stack by adjusting the stack pointer like in the previous code example.\nAs a detail, a more concise way could be to use push that combines the decrement of the SP (i.e. by 4 bytes) and the store of the operand (i.e. the integer 10) at the new address pointed to by the SP.\nThe base pointer The base pointer (BP) is set during function calls by copying the current SP. The BP is a snapshot of the SP at the moment of the function call (e.g. when the CPU fetches a call instruction), so that function parameters and local variables are accessed by adding and subtracting, respectively, a constant offset from it.\nMoreover when a new function is called a new space in the stack dedicated to the new function is created and some data like declaration of local variables is pushed.\nThis memory space dedicated to these subroutines are the stack frames, so each function will have a stack frame. You can find a simple scheme of stack frames with the main data pushed to the stack in the picture below:\nPlease bear in mind that the stack layout can vary based on the ABI calling convention and the architecture.\nWe\u0026rsquo;ll now go through the call path and see which data is also pushed to the stack, which is used to keep track of the execution path.\nThe call path When a new function is called the previous base pointer (BP) is also pushed to the new stack frame.\nWhile this is usually true, it\u0026rsquo;s not mandatory and it depends on how the binary has been compiled. This mainly depends on the compiler optimization techniques.\nIn particular, CALL instruction pushes also the value of the program counter at the moment of the new function call (next instruction address), and gives control to the target address. The program counter is set to the target address of the CALL instruction, which is, the first instruction of the called function.\nIn a nutshell: the just pushed return address is a snapshot of the program counter, and the pushed frame pointer is a snapshot of the base pointer, and they\u0026rsquo;re both available in the stack.\nAs a result, control is passed to the called subroutine address and the return address, that is the address of the instruction next to CALL, is available on the stack.\nThe following diagram wrap ups what\u0026rsquo;s been discussed until now:\nThe return path On the return path from the function, RET instruction POPs the return address from the stack and puts it in the program counter register. So, the next instruction is available from that return address.\nSince the program counter register holds the address of the next instruction to be executed, loading the return address into the PC effectively points the program execution to the instruction that follows the function call. This ensures the program resumes execution from the correct location after the function is completed.\nIn the case of a function calling a function, the program counter returns to the return address in the previous stack frame and starts executing from there.\nBecause all of the above points need to be memorized on the stack, the stack size will naturally increase, and on return decrease. And of course, the same happens to the stack and base pointers. Naturally, the stack is protected by a guard to avoid the stack overflow accessing unexpected area of memory.\nAs I\u0026rsquo;m a visual learner, the next section will show how the program\u0026rsquo;s code and data are organized in its process address space. This should give you a clearer picture of their layout within the process\u0026rsquo;s address space.\nThe address space regions The process address space is a logical view of memory managed by the operating system, hiding the complexity of managing physical memory.\nWhile explaining how memory mapping implementations work in operating systems is out of scope here, it\u0026rsquo;s important to say that user processes see one contiguous memory space thanks to the memory mapping features provided by the OS.\nThe address space is typically divided into different regions, and the following names are mostly standard between the operating systems:\n Text segment: this is the area where the (machine) code of the program is stored Data segment: this region contains typically static variables which are initialized BSS (Block Started by Symbol) segment: it contains global and static variables that are not initialized when the program starts. Because the data would be a block of zeros, the BSS content is omitted in the executable file, saving space. Instead, the program headers allow the loader to know how much space to allocate for the BSS section in virtual memory and it filled it out with zeros. That\u0026rsquo;s why, despite uninitialized data being data, is not placed in the data section. Heap: it\u0026rsquo;s a region available for dynamic allocation available to the running process. Programs can request pages from it at runtime (e.g. malloc from the C standard library). Stack: we already talked about it.  The next diagram will show the discussed memory regions starting from the physical perspective to the perspective of the single virtual address space of a program process:\n Credits for the diagram to yousha.blog.ir.\n The operating system can enforce protection for each of them, like marking the text section read-only to prevent modification of the running program\u0026rsquo;s instructions.\nWhen a program is loaded into memory, the operating system allocates a specific amount of memory for it and dedicates specific regions to static and dynamic allocation. The static allocation includes the allocation for the program\u0026rsquo;s instructions and the stack.\nDynamic allocations can be handled by the stack or the heap. The heap usually acquires memory from the bottom of the same region and grows upwards towards the middle of the same memory region.\nProgram loading in Unix-like OSes On program execution (Unix-like fork and exec system call groups) OS allocates memory to later store the program\u0026rsquo;s code and data. The exec family of system calls replaces the program executed by a process. When a process calls exec, all sections are replaced, including the .text section, and the data in the process are replaced with the executable of the new program.\nIn particular, the loader parses the executable file, decides which is the base address, allocates memory for the program segments based on the base address, loads the segments in memory, and prepares the execution environment.\nOnce the loader completes its tasks, it signals the kernel the program is ready. The kernel sets the process context and the PC to the first instruction in the .text section, which is fetched, decoded, and executed by the CPU.\n I haven\u0026rsquo;t managed yet to find where the information about how to set up the stack at exec time from an ELF file is stored in the ELF structure. If you do, feel free to share it!\n Moreover, as a detail, although all data is replaced, all open file descriptors remain open after calling exec unless explicitly set to close-on-exec.\nIf you want to go deeper on the Linux exec path, I recommend this chapter from the Linux insides book.\nNow let\u0026rsquo;s get back to the main characters of this blog, which are the pointer register. We mentioned that the base pointer is also called the frame pointer, indeed it points to a single stack frame. But, let\u0026rsquo;s see how they\u0026rsquo;re vital for CPU profiling.\nFrame pointer and the stack walking I\u0026rsquo;ve read more often the name frame pointer than base pointer, but actually the frame pointer is the base pointer.\nAs already discussed, the name base pointer comes to the fact that is set up when a function is called and is pushed to the new stack frame, to establish a fixed reference (base) to access local variables and parameters within the function\u0026rsquo;s stack frame.\nWhat is pushed to the stack are also the parameters, but depending on the ABI, they can be passed either on the stack or via registers. For instance:\n x86-64 System V ABI: in the general purpose registers rdi, rsi, rdx, rcx, r8, and r9 for the first six parameters. On the stack from the seventh parameter onward. i386 System V ABI: in the general purpose registers eax, ecx, edx, and ebx for the first four parameters. On the stack from the fifth parameter onward.  In general, the data that is commonly stored on the stack is:\n the return address the previous frame pointer saved register state the local variables of the function.   Remember: the return address is a snapshot of the program counter, so it points to instructions (code). The previous frame pointer is a snapshot of the base pointer, so it points to the stack (data).\n Below the local variables are other stack frames resulting from more recent function calls, as well as generic stack space used for computation and temporary storage. The most recent of these is pointed to by the stack pointer. This is the difference between the stack pointer and the frame/base pointer.\nHowever, the frame pointer is not always required. Compiler optimization technique can generate code that just uses the stack pointer.\nFrame pointer elimination (FPE) is an optimization that removes the need for a frame pointer under certain conditions, mainly to reduce the space allocated for the stack and to optimize performance because pushing and popping the frame pointer takes time during the function call. The compiler analyzes the function\u0026rsquo;s code to see if it relies on the frame pointer for example to access local variables, or if the function does not call any other function. At any point in code generation, it can determine where the return address, parameters, and locals are relative to the stack pointer address (either by a constant offset or programmatically).\nFrame pointer omission (FPO) is instead an optimization that simply instructs the compiler to not generate instructions to push and pop the frame pointer at all during function calls and returns.\n If you\u0026rsquo;re interested in the impacts of libraries compiled and distributed with this optimization I recommend the following Brendan Gregg\u0026rsquo;s great article: The Return of the Frame Pointers.\n Because the frame pointer is pushed on function call to the stack frame just created for the newly called function, and its value is the value of the stack pointer at the moment of the CALL, it points to the previous stack frame.\nA fundmental data needed by CPU profilers is to build stack traces, to understand the execution flow of a program and calculate the time spent for each trace and function.\nOne standard technique to build a stack trace is by walking fhe stack. And one technique to walk the stack is to follow the linked list of the saved frame pointers, beginning with the value hold by the base pointer register.\nBecause a RET (function returns) pops a stack frame out of the stack, when consequent RETs reach the top of the stack, which is the stack frame of the main function, a stack trace is complete. The same goes on and on with subsequent chains of call-returns that reach the top of the stack.\nYou can see it in the following picture a simplified scheme of the linked list of frame pointers:\nThis technique is particularly useful for profilers and debuggers. The following is a basic example of what a profiler could retrieve, leveraging frame pointers:\n$ ./my-profiler run --pid 12345 2.6% main.main;runtime.main;runtime.goexit; 65.3% main.foo;runtime.main;runtime.goexit; 32.1% main.bar;runtime.main;runtime.goexit; And this comes to the next episode of this series, which will dive into how to write a basic low-overhead, kernel-assisted CPU profiler leveraging eBPF, that will produce a result like the one above!\nI hope this has been interesting to you. Any feedback is more than appreciated.\nSee you in the next episode!\nReferences  https://www2.it.uu.se/edu/course/homepage/os/vt19/module-2/process-management/ https://stackoverflow.com/questions/18278803/how-does-elf-file-format-defines-the-stack https://stackoverflow.com/questions/21718397/what-are-the-esp-and-the-ebp-registers https://groups.google.com/g/golang-nuts/c/wtw0Swe0CAY https://www.polarsignals.com/blog/posts/2022/01/13/fantastic-symbols-and-where-to-find-them https://0xax.gitbooks.io/linux-insides/content/index.html https://blog.px.dev/cpu-profiling/  ","permalink":"https://blog.maxgio.me/posts/unleashing-power-frame-poiners-execution-environment/","summary":"Profiling the CPU allows us to analyze the program\u0026rsquo;s performance, identify bottlenecks, and optimize its efficiency.\nHave you ever wondered what happens behind the scenes when you run a program and how to account for CPU time for the actual program functions? And even more, how to write such a tool to profile the program?\nEven though great open-source projects provide continuous profiling with vast support for compiled, JITed, and interpreted, languages, with or without debug info, with or without frame pointers, etc.","title":"Unleashing the power of frame pointers for profilers pt.1 - The execution environment"},{"content":"Introduction wfind is a simple web crawler for files and folders in web pages hyerarchies. The goal is basically the same of GNU find for file systems. At the same time it\u0026rsquo;s inspired by GNU wget, and it merges the find features applied to files and directories exposed as HTML web resources.\nIn this blog we\u0026rsquo;ll go through the way I improved consistency in this crawler, by implementing retry logics and tuning network and transport in the HTTP client.\nParallelism and concurrency As a crawler, wfind is vital to efficiently do its work scraping web pages in parallel routines.\nFor scraping web pages wfind leverages go-colly, that allows run its collector in asynchronous mode. That mode simply fetches HTTP objects inside dedicated goroutines.\nFrom the user perspective (i.e. wfind), the synchronization is as simple as invoking the Wait function before completing. The API is provided by the Colly collector and it wraps around the standard WaitGroup\u0026rsquo;s Wait(), from the Go standard library\u0026rsquo;s sync package, waiting for all the fetch goroutines to complete.\nAs the go-colly implementation does not provide cap on the parallelism, the implementation can lead to the common concurrency problems, racing for OS and runtime resources client-side, server-side, and physical medium-side.\nClient-side, the maxmimum allowed open connections could prevent the client to open and then establish new ones during the scraping. The server could limit resource usage and we cannot predict the strategies and logics followed server-side. Also, the connection mean in the physical layer is another point of failure; for example latency might cause the HTTP client to time out during go-colly\u0026rsquo;s Visit waiting for a response.\nAt the end of the day a retry logics was fundamental in order to improve the consistency in the crawling. Furthermore, verifying the consistency through end-to-end functional tests is required to guarantee the expected behaviour of the program.\nEnd-to-end tests As end-to-end functional tests treat the program as a black-box and ensures that provide the value as expected, interacting with the real actors in the expected scenarios, I wrote tests again real CentOS kernel.org mirrors, looking for repository metadata files, as an example use case of wfind.\nI used GinkGo as I like how it easily enables to design and implement the specifications of the program as you write tests.\nMoreover, regardless of whether or not you follow BDD, tests tend to appear self-explanatory.\nIndeed, Ginkgo with Gomega matchers provide a DSL for writing tests in general like integration tests but also white-box and black-box unit tests.\npackage find_test import ( . \u0026#34;github.com/onsi/ginkgo/v2\u0026#34; . \u0026#34;github.com/onsi/gomega\u0026#34; \u0026#34;github.com/maxgio92/wfind/internal/network\u0026#34; \u0026#34;github.com/maxgio92/wfind/pkg/find\u0026#34; ) const ( seedURL = \u0026#34;https://mirrors.edge.kernel.org/centos/8-stream\u0026#34; fileRegexp = \u0026#34;repomd.xml$\u0026#34; expectedResults = 155 ) var _ = Describe(\u0026#34;File crawling\u0026#34;, func() { Context(\u0026#34;Async\u0026#34;, func() { var ( search = find.NewFind( find.WithAsync(true), find.WithSeedURLs([]string{seedURL}), find.WithFilenameRegexp(fileRegexp), find.WithFileType(find.FileTypeReg), find.WithRecursive(true), ) actual *find.Result err error expectedCount = expectedResults ) BeforeEach(func() { actual, err = search.Find() }) It(\u0026#34;Should not fail\u0026#34;, func() { Expect(err).To(BeNil()) }) It(\u0026#34;Should stage results\u0026#34;, func() { Expect(actual.URLs).ToNot(BeEmpty()) Expect(actual.URLs).ToNot(BeNil()) }) It(\u0026#34;Should stage exact result count\u0026#34;, func() { Expect(len(actual.URLs)).To(Equal(expectedCount)) }) }) }) As you can see, the order in which results are returned is not important and thus not tested.\nRetry logics The first concrete goal of the retry logics was to start to see green flags from the GinkGo output.\nSo I expected to start by seeing tests to fail:\n$ ginkgo --focus \u0026#34;File crawling\u0026#34; pkg/find ... FAIL! ... Then, in order to make tests to pass, it was needed a way to ensure that requests failed would have been retried.\nFortunately go-colly provide way to register a callback, that as per the documentation it registers a function that will be executed if an error occurs during the HTTP request, with OnError.\nThat way it\u0026rsquo;s possible to run a custom handler as the response (and the request) object and the error are available in context of the helper, as for the signature.\nDumb retrier The first implementation of the retry could have been as simple as retry for a fixed amount of times, after a fixed amount of period.\nFor example:\ncollector.OnError(func(resp *colly.Response, err error) { time.Sleep(2 * time.Second) resp.Request.Retry() }) For sure this wasn\u0026rsquo;t enough to improve the probability to make failing requests to succeed.\nRetry with exponential backoff At first, a single retry might not be enough, and also, the optimal backoff size should vary depending on the failure cause and the context. Furthermore, it would be good to be increased as time passes in order to avoid overload on the actors.\nSo I decided to leverage the community projects and digging around backoff implementations. After that, I picked and imported github.com/cenkalti/backoff package. I liked the design as it respects all the SOLID principles and because it provides API to a tunable exponential backoff algorithm. Also, it allows to mix and match with different custom backoff algorithms, without needing to implement a ticker.\nFurthermore, I wanted to provide knobs to enable the retry behaviour for specific errors encountered doing HTTP requests. So I ended up including new dedicated options to the wfind/pkg/find\u0026rsquo;s ones:\npackage find // ...  // Options represents the options for the Find job. type Options struct { // ...  // ConnResetRetryBackOff controls the error handling on responses. \t// If not nil, when the connection is reset by the peer (TCP RST), the request \t// is retried with an exponential backoff interval. \tConnResetRetryBackOff *ExponentialBackOffOptions // TimeoutRetryBackOff controls the error handling on responses. \t// If not nil, when the connection times out (based on client timeout), the request \t// is retried with an exponential backoff interval. \tTimeoutRetryBackOff *ExponentialBackOffOptions // ContextDeadlineExceededRetryBackOff controls the error handling on responses. \t// If not nil, when the request context deadline exceeds, the request \t// is retried with an exponential backoff interval. \tContextDeadlineExceededRetryBackOff *ExponentialBackOffOptions } // ...  // crawlFiles returns a list of file names found from the seed URL, // filtered by file name regex. func (o *Options) crawlFiles() (*Result, error) { // Create the collector. \tco := colly.NewCollector(coOptions...) // Add the callback to Visit the linked resource, for each HTML element found \tco.OnHTML(HTMLTagLink, func(e *colly.HTMLElement) { // ... \t}) // Manage errors. \tco.OnError(o.handleError) // ...  // Wait until colly goroutines are finished. \tco.Wait() return \u0026amp;Result{BaseNames: files, URLs: urls}, nil } // handleError handles an error received making a colly.Request. // It accepts a colly.Response and the error. func (o *Options) handleError(response *colly.Response, err error) { switch { // Context timed out. \tcase errors.Is(err, context.DeadlineExceeded): if o.ContextDeadlineExceededRetryBackOff != nil { retryWithExponentialBackoff(response.Request.Retry, o.TimeoutRetryBackOff) } // Request has timed out. \tcase os.IsTimeout(err): if o.TimeoutRetryBackOff != nil { retryWithExponentialBackoff(response.Request.Retry, o.TimeoutRetryBackOff) } // Connection has been reset (RST) by the peer. \tcase errors.Is(err, unix.ECONNRESET): if o.ConnResetRetryBackOff != nil { retryWithExponentialBackoff(response.Request.Retry, o.ConnResetRetryBackOff) } // Other failures. \tdefault: // ... \t} } With the implementation of the retry leveraging the cenkalti/backoff package, following the example provided:\n// retryWithExtponentialBackoff retries with an exponential backoff a function. // Exponential backoff can be tuned with options accepted as arguments to the function. func retryWithExponentialBackoff(retryF func() error, opts *ExponentialBackOffOptions) { ticker := backoff.NewTicker( utils.NewExponentialBackOff( utils.WithClock(opts.Clock), utils.WithInitialInterval(opts.InitialInterval), utils.WithMaxInterval(opts.MaxInterval), utils.WithMaxElapsedTime(opts.MaxElapsedTime), ), ) var err error // Ticks will continue to arrive when the previous retryF is still running, \t// so operations that take a while to fail could run in quick succession. \tfor range ticker.C { if err = retryF(); err != nil { // Retry. \tcontinue } ticker.Stop() break } if err != nil { // Retry has failed. \treturn } // Retry is successful. } And the end-to-end test could have been updated by enabling the retry behaviour for the context deadline exceeded, HTTP client transport\u0026rsquo;s timeout, connection reset by peer cases:\nvar _ = Describe(\u0026#34;File crawling\u0026#34;, func() { Context(\u0026#34;Async\u0026#34;, func() { var ( search = find.NewFind( find.WithAsync(true), find.WithSeedURLs([]string{seedURL}), find.WithClientTransport(network.DefaultClientTransport), find.WithFilenameRegexp(fileRegexp), find.WithFileType(find.FileTypeReg), find.WithRecursive(true), // Enable retry backoff with default parameters. \tfind.WithContextDeadlineExceededRetryBackOff(find.DefaultExponentialBackOffOptions), find.WithConnTimeoutRetryBackOff(find.DefaultExponentialBackOffOptions), find.WithConnResetRetryBackOff(find.DefaultExponentialBackOffOptions), ) actual *find.Result err error expectedCount = expectedResults ) BeforeEach(func() { actual, err = search.Find() }) It(\u0026#34;Should not fail\u0026#34;, func() { Expect(err).To(BeNil()) }) It(\u0026#34;Should stage results\u0026#34;, func() { Expect(actual.URLs).ToNot(BeEmpty()) Expect(actual.URLs).ToNot(BeNil()) }) It(\u0026#34;Should stage exact result count\u0026#34;, func() { Expect(len(actual.URLs)).To(Equal(expectedCount)) }) }) }) And I re-run the e2e test again:\n$ ginkgo --focus \u0026#34;File crawling\u0026#34; pkg/find But the tests took too much time consuming a lot of memory until it was out-of-memory killed. Likely a memory leak or simply not efficient memory management was already present, but without retry logics nor performance tests it hadn\u0026rsquo;t shown up.\nSo, a heap memory profile for the find run was then needed. The run specifics of the end-to-end test in example was enough.\nMemory profiling: entering pprof Long story short, pprof is a standard library\u0026rsquo;s package that serves via its HTTP server runtime profiling data in the format expected by the pprof visualization tool.\n I recommend the official documentation of the package, and this great Julia Evans' blog.\n So, I simply linked pprof package:\npackage find import ( /// ...  _ \u0026#34;net/http/pprof\u0026#34; // ... ) modified the tested function to run in parallel its webserver:\npackage find // ...  func (o *Options) Find() (*Result, error) { go func() { log.Println(http.ListenAndServe(\u0026#34;localhost:6060\u0026#34;, nil)) }() if err := o.Validate(); err != nil { return nil, errors.Wrap(err, \u0026#34;error validating find options\u0026#34;) } switch o.FileType { case FileTypeReg: return o.crawlFiles() case FileTypeDir: return o.crawlFolders() default: return o.crawlFiles() } } and finally run the tests again:\n$ ginkgo --focus \u0026#34;File crawling\u0026#34; pkg/find and immediately invoke the pprof go tool to download the heap memory profile as a PNG image:\n$ go tool pprof http://localhost:6060/debug/pprof/heap (pprof) png Generating report in profile001.png Looking at the profile function call graph it was evident that a great amount of memory mapping was request by a reading: io.ReadAll(), called from colly.Do():\nSo digging into the go-colly HTTP backend Do implementation, the offending line was:\npackage colly //...  func (h *httpBackend) Do(request *http.Request, bodySize int, checkHeadersFunc checkHeadersFunc) (*Response, error) { // ... \tres, err := h.Client.Do(request) if err != nil { return nil, err } defer res.Body.Close() // ... \tvar bodyReader io.Reader = res.Body if bodySize \u0026gt; 0 { bodyReader = io.LimitReader(bodyReader, int64(bodySize)) } // ... \tbody, err := ioutil.ReadAll(bodyReader) // ... } So, a first solution was to limit the size of the response body which was being read.\nMax HTTP body size Fortunately, go-colly provides a way to set the requests' maximum body size that will be read, so I ended up exposing an option:\npackage find // Options represents the options for the Find job. type Options struct { // ... \t// MaxBodySize is the limit in bytes of each of the retrieved response body. \tMaxBodySize int // ... } which then would have fill the colly collector setting:\npackage find // ...  // crawlFiles returns a list of file names found from the seed URL, filtered by file name regex. func (o *Options) crawlFiles() (*Result, error) { // ...  // Create the collector settings \tcoOptions := []func(*colly.Collector){ // ... \tcolly.MaxBodySize(o.MaxBodySize), } Finally I updated the end-to-end test, tuning the parameter with an expected maximum value, considering the HTML nature of expected response body:\nvar _ = Describe(\u0026#34;File crawling\u0026#34;, func() { Context(\u0026#34;Async\u0026#34;, func() { var ( search = find.NewFind( find.WithAsync(true), find.WithSeedURLs([]string{seedURL}), find.WithFilenameRegexp(fileRegexp), find.WithFileType(find.FileTypeReg), find.WithRecursive(true), find.WithMaxBodySize(1024*512), find.WithConnTimeoutRetryBackOff(find.DefaultExponentialBackOffOptions), find.WithConnResetRetryBackOff(find.DefaultExponentialBackOffOptions), ) actual *find.Result err error expectedCount = expectedResults ) BeforeEach(func() { actual, err = search.Find() }) It(\u0026#34;Should not fail\u0026#34;, func() { Expect(err).To(BeNil()) }) It(\u0026#34;Should stage results\u0026#34;, func() { Expect(actual.URLs).ToNot(BeEmpty()) Expect(actual.URLs).ToNot(BeNil()) }) It(\u0026#34;Should stage exact result count\u0026#34;, func() { Expect(len(actual.URLs)).To(Equal(expectedCount)) }) }) } and run again the tests:\n$ ginkgo --focus \u0026#34;File crawling\u0026#34; pkg/find ... Ran 3 of 3 Specs in 7.552 seconds SUCCESS! -- 3 Passed | 0 Failed | 0 Pending | 0 Skipped Now tests passed in just less than 8 seconds!\nMore tuning: HTTP client\u0026rsquo;s Transport Another important network and connection parameters are provided with the go net/http Transport. Connection timeout, TCP keep alive interval, TLS handshake timeout, Go net/http idle connnection pool maximum size, idle connections timeout are just some of them.\nThe connection pool size here is fundamental to be tuned in order to satisfy the level of concurrency enabled by the asynchronous mode of go-colly, hence of wfind.\nIn detail, Go net/http Get keeps the connection pool as a cache of TCP connections, but when all are in use it opens another one. If the parallelism is greater than the limit of idle connections, the program is going to be regularly discarding connections and opening new ones, the latters ending up in TIME_WAIT TCP state for two minutes, tying up that connection.\n About TIME_WAIT TCP state I recommend this blog by Vincent Bernat.\n From the Go standard library net/http package:\npackage http type Transport struct { // ...  // MaxIdleConns controls the maximum number of idle (keep-alive) \t// connections across all hosts. Zero means no limit. \tMaxIdleConns int // MaxIdleConnsPerHost, if non-zero, controls the maximum idle \t// (keep-alive) connections to keep per-host. If zero, \t// DefaultMaxIdleConnsPerHost is used. \tMaxIdleConnsPerHost int As so, it was very useful to provide way to inject a client Transport configured for specific use cases:\npackage find // Options represents the options for the Find job. type Options struct { // ...  // ClientTransport represents the Transport used for the HTTP client. \tClientTransport http.RoundTripper // ... } and in the go-colly collector to set up the client with the provided Transport:\npackage find // crawlFiles returns a list of file names found from the seed URL, filtered by file name regex. func (o *Options) crawlFiles() (*Result, error) { ... // Create the collector settings \tcoOptions := []func(*colly.Collector){ colly.AllowedDomains(allowedDomains...), colly.Async(o.Async), colly.MaxBodySize(o.MaxBodySize), } ... // Create the collector. \tco := colly.NewCollector(coOptions...) if o.ClientTransport != nil { co.WithTransport(o.ClientTransport) } Wrapping up As wfind main command is the first consumer, from its perspective, the command Run would consume it as so:\nfunc (o *Command) Run(_ *cobra.Command, args []string) error { ... // Network client dialer. \tdialer := network.NewDialer( network.WithTimeout(o.ConnectionTimeout), network.WithKeepAlive(o.KeepAliveInterval), ) // HTTP client transport. \ttransport := network.NewTransport( network.WithDialer(dialer), network.WithIdleConnsTimeout(o.IdleConnTimeout), network.WithTLSHandshakeTimeout(o.TLSHandshakeTimeout), network.WithMaxIdleConns(o.ConnPoolSize), network.WithMaxIdleConnsPerHost(o.ConnPoolPerHostSize), ) // Wfind finder. \tfinder := find.NewFind( find.WithSeedURLs(o.SeedURLs), find.WithFilenameRegexp(o.FilenameRegexp), find.WithFileType(o.FileType), find.WithRecursive(o.Recursive), find.WithVerbosity(o.Verbose), find.WithAsync(o.Async), find.WithClientTransport(transport), ) for which default command\u0026rsquo;s flag default values are provided by wfind for its specific use case.\nConclusion The retry logics allowed to provide consistency, and network and transport tuning in the HTTP client improved the efficiency and performance.\nAs usual, there\u0026rsquo;s alwasy something to learn and it\u0026rsquo;s cool how deep we can dig into things. I was curious about the reason why so much connections in TIME_WAIT state were left during the scraping, even if they\u0026rsquo;re not a problem. So learning how Go runtime manages the connections keeping a cache pool of them was the key to understand more and how to optimize the management in cases like this, where there may be high parallalism and probably high concurrency as well, on OS network stack\u0026rsquo;s resources.\nMoreover, I like Go every day more, as already the standard library provides often all you need with primitives, and in this case for network and for synchronization.\nThank you! I hope this was interesting for you as it was for me. Please, feel free to reach out!\nTwitter Mastodon Github Linkedin\n","permalink":"https://blog.maxgio.me/posts/improving-consistency-performance-go-crawler-retry-logics-http-client-tuning/","summary":"Introduction wfind is a simple web crawler for files and folders in web pages hyerarchies. The goal is basically the same of GNU find for file systems. At the same time it\u0026rsquo;s inspired by GNU wget, and it merges the find features applied to files and directories exposed as HTML web resources.\nIn this blog we\u0026rsquo;ll go through the way I improved consistency in this crawler, by implementing retry logics and tuning network and transport in the HTTP client.","title":"How I improved consistency and performance in a Go crawler with retry logics and network tuning"},{"content":"Two years ago more or less I started my journey in Linux. I was scared at first and I didn\u0026rsquo;t know where to start from. But then I decided to buy a book - and what a book! - in order to follow a path.\nAlong the way, I integrated the material with up-to-date documentation from kernel.org and source code. In the meantime, I started to learn C a bit so that I also could have played with what I was learning, step by step.\nOne of the things I was fascinated by was how Linux is able to manage and let the CPU run thousands and thousands of processes each second. To give you an idea, right now, Linux on my laptop configured with an Intel i7-1185G7 CPU switched context 28,428 times in a second! That’s fantastic, isn’t it?\n$ perf stat -e sched:sched_switch --timeout 1000 Performance counter stats for \u0026#39;system wide\u0026#39;: 28,428 sched:sched_switch 1.001137885 seconds time elapsed During this journey inside Linux, I\u0026rsquo;ve written notes as it helps me to digest and re-process in my own way the informations I learn. Then I thought: \u0026ldquo;Maybe they\u0026rsquo;re useful to someone. Why not share them?”.\nSo here I am with with a blog.\n 1. Resource sharing is the key! Let’s dive into the Linux component which is responsible for doing such great work: the scheduler.\nIn order to do it, imagine what we would expect from an operating system. Let\u0026rsquo;s say that we’d want it to run tasks that we need to complete, providing the OS hardware resources. Tasks come of different natures but we can simply categorize them as CPU-intensive and interactive ones.\nSomething should provide the efficiency of task completion and responsiveness. Consider a typewriter that prints letters with 1s second of delay, it would be impossible to use! So, in a few words, I would like to request to the scheduler: “I want to execute this task and I want it’s completed when I need or to respond when I need”. The goal of a scheduler is to decide “what runs next” leading to have the best balance between the needings of the different natures of the tasks.\nAs Linux is a preemptive multitasking operating system, the completely fair scheduler (CFS) came to Linux, as the replacement of the O(1) scheduler from the 2.6.23, with the aim to guarantee the fairness of CPU owning by the tasks, and at the same time tailoring to a broad nature range of tasks. Although the algorithm complexity didn\u0026rsquo;t see an improvement, from O(1) to O(log N), the reduced latency removed issues when dealing with interactive tasks.\nAs a side note consider that the Linux scheduler is made of different scheduler classes (code), of which the CFS class is the highest-priority one. Another one is the real-time scheduler class, tailored as the name suggests for tasks that need responsiveness.\nInteractive tasks would run for small amounts of time but need to run quickly as events happen. CPU-intensive tasks don’t require to complete ASAP but require longer CPU time. Based on that, time accounting is what guarantees fairness in the Linux CFS scheduler as long as the task that runs for less time will run next.\nThis comes to the time accounting, so let\u0026rsquo;s start to dig into it!\n 2. Time accounting Linux CFS actually does not directly assign timeslices to tasks as the O(1) scheduler did, instead it measures execution time, in order to be flexible with respect to both interactive and processor-intensive tasks.\nThe runtime Remember, the fundamental rule in the Completely Fair Scheduler is: the task that ran less, will run next! Which is, each task should have its fair slice of processor time when it needs! For example, interactive tasks can run frequently but for less time than intensive ones, and still have their fair amount of CPU time.\nThe implementation is written in the update_curr() function, which is called periodically to account tasks for the CPU time they used in the last period (delta_exec).\nThe virtual runtime The execution time is further weighted to implement priority between tasks. This is done by the fair delta calculation. The more the weight, the more time the task will have.\nExample Let' do an example with timeslices: considering a single CPU, if every T time period two tasks A and B run respectively with a weight of 1 and 2, the allocated CPU time is obtained by multiplying T by the ratio of the weight to the sum of the weights of all running tasks:\nCPU_timeslice(A) = T * (1 / (1 + 2))). CPU_timeslice(B) = T * (2 / (1 + 2))). For each T time period, task A will run for 0.334~T and task B 0.667~T.\n This is what is calculated here.\n Implementation Coming to the actual implementation, the CFS class accounts tasks for their real execution time considering their weight, which is ensured by periodically measuring the runtime and multiplying it by the ratio weight/(base weight).\nruntime += runtime * (w / base w)). Which is exactly what is done in update_curr():\nstatic void update_curr(struct cfs_rq *cfs_rq) { struct sched_entity *curr = cfs_rq-\u0026gt;curr; ... delta_exec = now - curr-\u0026gt;exec_start; ... curr-\u0026gt;vruntime += calc_delta_fair(delta_exec, curr); ... } And the result is the so-called virtual runtime (vruntime).\nAs the weight implementation depends on the nature of the schedule entities, let\u0026rsquo;s spend a couple of words about them.\n Indeed, the vruntime is a member of the sched_entity structure.\n Then, we\u0026rsquo;ll talk more about the runtime weight.\nThe schedule entities Until now we talked about tasks as the only schedulable entity but actually, tasks can be put into group of tasks, in order to treat a group equally to a single task, and have the group share the resources (I.e. CPU) between the entities of the group without afflicting the overall system. That’s the case of cgroups and why they’re there.\nAlso, task groups can be composed of other groups, and there is a root group. In the end, a running Linux is likely going to manage a hierarchy tree of schedule entities. So when a task should be accounted for time, also the parent group\u0026rsquo;s entity should be, and so on, until the root group entity is found.\n Consider that the sched_entity structure is the structure that tracks information about the scheduling, like the vruntime, and it refers to tasks or tasks group structures. As they track scheduling data, they are per-CPU structures. Instead, task_struct and task_group structures, are global.\n And this comes to the weight.\nThe weight We said before that the weight implementation depends on the nature of the entity. If the entity is a task the weight is represented by the niceness value (code). If it’s a task group, the weight is represented by the CPU shares value.\n In cgroup v2 the shares is named directly weight.\n In the end, the weight is what matters: in the case of tasks the niceness is converted to priority and then to weight (here). In the case of task groups the user-visible value is internally converted.\nFor the sake of simplicity let’s remember this: the groups are hierarchical, and a task is part of a task group. The bigger the depth of the hierarchy, the more the weight gets diluted. Adding a heavily weighted task to one child group is not going to afflict the overall tasks tree the same as it would do if it was part of the root group. This is because the task weight is relative to the group which the task is put into.\nEach entity, whether a task or a task group, is treated the same. The time accounting is applied to the currently locally running entity and recursively up through the hierarchy.\n In case of task groups, the weight is further scaled, but don\u0026rsquo;t worry, we\u0026rsquo;ll talk about it later.\n Update of the virtual runtime This virtual runtime is updated on the schedule entity that is currently running on the local CPU via update_curr() function, which is called:\n whenever a task becomes runnable, or whenever blocks become unrunnable, and periodically (every 1/CONFIG_HZ seconds) by the system timer interrupt handler.   As a detail, the virtual runtime value if the task is just forked, is initialized to a minimum value which depends on the runqueue load (cfs_rq-\u0026gt;min_vruntime).\n And this leads to the next question: how this accounting is honored in the task selection in the scheduler in order to guarantee fairness of execution?\n 3. Task selection The schedule entities eligible to run (which is in a runnable state) are put in a run queue, which is implemented as a red black self-balancing binary tree that contains schedule entity structures ordered by vruntime.\nThe runqueues Runqueues are per-CPU structures and contain schedule entities and they have a pointer to the entity which is currently running on the related CPU. The schedule entities they refer to are related to the local CPU because the sched_entitys contain information about scheduling and thus are specific to a CPU. The vruntime is the binary tree key so the entity with the smallest vruntime is picked during a new schedule.\n Each scheduler class has its specific runqueue, which are part of the general runqueues. Anyway, let\u0026rsquo;s consider now only CFS runqueues.\n In turn, also each task group has a dedicated CFS runqueue, from the root task group through its child task groups. __pick_next_entity() picks the entity with the smallest virtual runtime, whether is an actual task or a group. If it’s a task group the search is repeated on its runqueue and so on, going through the hierarchy of runqueues until a real task is found to be run.\nEach runqueue keeps track of the schedule entities that are running/runnable on the local CPU.\nIn a nutshell  Tasks groups are global. Tasks also are global. Every task is part of a task group. There is one runqueue per task group per CPU. Runqueues are composed of schedule entities. Schedule entities reference tasks or task groups. Schedule entities are per CPU  Wrapping up the structures To make it more clear, let\u0026rsquo;s see a practical example. You can see below a diagram for a sample scenario where there are two tasks (p1 and p2), and two task groups (root task group and tg1, child of the root task group). And p1 is direct child of task group tg1 and p2 is direct child of the root task group. i is the i-th CPU: Global structures  task_group.se: se[i] is the task groups\u0026rsquo;s sched_entity data for i-th CPU. task_group.cfs_rq: cfs_rq[i] is the task group\u0026rsquo;s cfs_rq data for i-th CPU. task_group.parent: the parent task group. task_group.shares: the task group cpu.shares task_struct.sched_class: the scheduler class the tasks should be scheduled with.  Per-CPU structures  sched_entity.vruntime: the virtual runtime. sched_entity.parent: the schedule entity of the parent task group. sched_entity.my_q: when not a task (NULL), the task group\u0026rsquo;s CFS runqueue on the local CPU. sched_entity.run_node: the related red-black tree node on the runqueue tree. sched_entity.cfs_rq: the CFS runqueue that manages the schedule entity sched_entity.load: the weight of the entity. If it relates to a task group, is the sum of the weights of the tasks of the group, on the local CPU. cfs_rq.load: the load of the runqueue, aka the sum of the weights of the entities that compose it. cfs_rq.current: the schedule entity that is currently running on the local CPU, where a group or a task. cfs_rq.rq: the general CPU runqueue to which the CFS runqueue is attached. cfs_rq.tg: the task group that owns the runqueue, whether the root one or a child. rq.cfs_tasks: the linked list containing the reb-black tree nodes (e.g. here CFS puts the next entity into it).   If you would like to explore the relations between the entities, I recommend this blog.\n Now that we introduced runqueues, let\u0026rsquo;s talk about the further scaling of the runtime weight for task groups schedule entities.\nWeight for task groups As task groups can be run on multiple CPUs doing real multitasking, the weight (i.e. CPU shares) for task group\u0026rsquo;s runqueue is further updated (scaled) in entity_tick() based on how much the task group is loaded on the local CPU.\nThe weight is multiplied by the ratio of the load of the task group running on the local CPU (which is the task group\u0026rsquo;s runqueue) to the global load of the task group.\nThis ratio tells us how much the task group is loaded on the local CPU.\n As a detail, this is done if configured Linux for symmetrical multiprocessor, otherwise the shares is not scaled.\n In detail, the load is the sum of the weights of the entities that compose the task group or the task group\u0026rsquo;s runqueue.\nshares = shares * (runqueue's load / task group's load) TL;DR: the ratio is the sum of the weights of the entities that compose the runqueues to the sum of the weights of the entities that compose the task group:\nThe calculcation is done by the calc_group_shares() function, to get the final value of the task group\u0026rsquo;s shares that will weight the virtual runtime of the task group schedule entity:\nstatic long calc_group_shares(struct cfs_rq *cfs_rq) { long tg_weight, tg_shares, load, shares; / struct task_group *tg = cfs_rq-\u0026gt;tg; /* tg_shares is the task group's CPU shares. */ tg_shares = READ_ONCE(tg-\u0026gt;shares); /* load is the load of the local CFS runqueue which is, the load of the task group on the local CPU. */ load = max(scale_load_down(cfs_rq-\u0026gt;load.weight), cfs_rq-\u0026gt;avg.load_avg); /* tg_weight is the global load of the task group. */ tg_weight = atomic_long_read(\u0026amp;tg-\u0026gt;load_avg); /* Ensure tg_weight \u0026gt;= load */ tg_weight -= cfs_rq-\u0026gt;tg_load_avg_contrib; tg_weight += load; shares = (tg_shares * load); if (tg_weight) shares /= tg_weight; // ... /* shares is now the per CPU-scaled task group shares. */ return clamp_t(long, shares, MIN_SHARES, tg_shares); } This is done to treat fairly also groups among CPUs!\n... shares = (tg_shares * load); if (tg_weight) shares /= tg_weight; ... Consequently, the vruntime is the binary tree key so the entity with the smallest vruntime is picked by __pick_next_entity(), whether is an actual task or a group. If it’s a task group the search is repeated on its runqueue and so on, going through the hierarchy of runqueues until a real task is found to be run.\n As a detail, in order to provide efficiency and to not need to traverse the whole tree every time a scheduling is needed, as the element in an ordered red black tree that is leftmost is the element with a minor key value (i.e. the vruntime) a cache is easily keeped as rb_leftmost variable in each runqueue structure. And it\u0026rsquo;s ideally picked by pick_next_entity().\n Finally, we can better see the whole picture!\nWrapping up the time accounting Now that we have the most important concepts in mind about time accounting, considering how the weight is calculated for both tasks and tasks groups schedule entities, which are part of hierarchical tasks groups' runqueues, let\u0026rsquo;s see how the time accounting is honored during the periodic tick, fired by the timer interrupt:\n/* * Called by the timer interrupt handler every 1/HZ seconds. */ scheduler_tick() /* The local CPU */ -\u0026gt; int cpu = smp_processor_id(); /* The local CPU runqueue */ -\u0026gt; struct rq *rq = cpu_rq(cpu); /* The local CPU runqueue currently running task */ -\u0026gt; struct task_struct *curr = rq-\u0026gt;curr; /* The current running task's scheduler class' periodic tick function. */ -\u0026gt; curr-\u0026gt;sched_class-\u0026gt;task_tick(rq, curr) /* Let's assume the class is CFS. */ -\u0026gt; task_tick_fair() -\u0026gt; struct sched_entity *se = \u0026amp;curr-\u0026gt;se; /* * For each schedule entity through parent task groups * up to the root task group. */ -\u0026gt; for_each_sched_entity(se) /* The runqueue where the entity is placed. */ -\u0026gt; cfs_rq = cfs_rq_of(se); -\u0026gt; entity_tick(cfs_rq, se) /* * Update the virtual runtime for the current running entity * on the current selected by loop-task group's runqueue. */ -\u0026gt; update_curr(cfs_rq) -\u0026gt; struct sched_entity *curr = cfs_rq-\u0026gt;curr; -\u0026gt; delta_exec = now - curr-\u0026gt;exec_start; -\u0026gt; curr-\u0026gt;exec_start = now; -\u0026gt; curr-\u0026gt;vruntime += calc_delta_fair(delta_exec, curr); /* * If it's a task group update the shares * based on its group runqueue, * which is the group load on the local CPU */ -\u0026gt; update_cfs_group(se) /* The CFS runqueue of the entity, if it's a task group. */ -\u0026gt; struct cfs_rq *gcfs_rq = group_cfs_rq(se); /* If the entity is a task, skip */ -\u0026gt; if (!gcfs_rq) -\u0026gt; return; /* * Update the CPU shares for the task group entity. */ -\u0026gt; shares = calc_group_shares(gcfs_rq); -\u0026gt; reweight_entity(cfs_rq_of(se), se, shares);  The code has been a bit simplified to show a clearer picture.\n So, the next question is: how a runqueue is populated? When a new task is added do a runqueue?\nRunqueues population The runqueues are populated when:\n a clone() is called, and a task wakes up after having slept, via try_to_wake_up() function call  with enqueue_entity().\nAnd the next question is: when a task is removed from the runqueue?\n When a task explicitly exit()s (e.g. via exit() libc function) When a task explicitly or implicitly requests to sleep()  with dequeue_entity().\nIn both enqueue and dequeue cases the rb_leftmost cache is updated and replaced with rb_next() result.\nNow that we have a runqueue populated, how does the scheduler pick one task from there?\nThe scheduler entrypoint schedule() is the main function which (through __schedule), calls pick_next_task that will return the task that ran less.\nFor the sake of simplicity, let\u0026rsquo;s assume that the hyperthreading support is not configured.\n More on core scheduling here.\n __pick_next_task() picks the highest priority scheduler class which returns the higher priority task, by looping through the hierarchy of task groups' runqueues, until a real task is found. Actually, as we said before, the runqueue red-black trees are not traversed on each schedule, instead, in the end, it picks the rb_leftmost entity rb node, through __pick_next_entity.\nschedule() loops while the currently running task should be rescheduled, which is, is no longer fair to be run.\n The path is a bit different when the core scheduling feature is enabled.\n Then, __schedule() calls context_switch() that switches to the returned task.\nAnd this comes to one of the next topics: context switch. But before talking about that let’s continue talking about the life of a task.\nLet’s imagine that we are in process context and that our task is now running. Not all tasks complete from the time have being scheduled. For example, tasks waiting for events (like for keyboard input or for file I/O) can be put to sleep, and also are in interruptible / uninterruptible state so they aren’t picked from the runqueue.\n 4. Sleep and wake up A task can decide to sleep but something then is needed to wake it up. We should also consider that multiple tasks can wait for some event to occur.\nA wait queue of type wait_queue_head is implemented for this purpose as a doubly-linked list of tasks waiting for some events to occur. It allows tasks to be notified when those events occur by referencing the wait queue, generally from what generates the event itself.\nSleep A task can put itself to sleep in the kernel similar to what does the wait syscall:\n create a wait queue via the DECLARE_WAIT_QUEUE_HEAD() macro add the task itself to it via add_wait_queue() function call set its state to interruptible / uninterruptible via prepare_to_wait(). If the task is set interruptible, signals can wake it up. call schedule() which in turn removes the task from the runqueue via deactivate_task().  /* ‘q’ is the wait queue we wish to sleep on */ DEFINE_WAIT_QUEUE_HEAD(wait); add_wait_queue(q, \u0026amp;wait); while (!condition) { /* condition is the event that we are waiting for */ prepare_to_wait(\u0026amp;q, \u0026amp;wait, TASK_INTERRUPTIBLE); ... schedule(); } It can also do it non voluntarily waiting for semaphores.\n As a detail, wait queues have two implementations, the one we mentioned above and the original one (Linux 2.0) which has been kept for simple use cases and is called now simple wait queues (more on the history here).\n Wake up For the sake of simplicity on the path to waking up let’s take the example of the simple wait queue, as the standard wait queue here is more complex than it is in the preparation to wait, and we don\u0026rsquo;t need to understand it now.\nTo wake those tasks that are sleeping while waiting for an event here is the flow:\nswake_up_all() (which is pretty analogous to the sibling implementation\u0026rsquo;s  wake_up_all()) calls try_to_wake_up() and is there to wake all processes in a wait queue when the associated event occurs.\ntry_to_wake_up() does the work that consists of:\n set task state to running - and through ttwu_queue: calls the activate_task() function which adds the task to the runqueue via enqueue_task() sets need_resched flag on the current task if the awakened task has higher priority than the current one (we’ll talk about this flag later) which provokes then a schedule() (and consequent context switch)  swake_up_all() then removes the task from the wait queue.\nSignals Signals, as well, can wake up tasks if they are interruptible. In this case, the task code itself should then manage the spurious wake-up (example), by checking the event that occurs or managing the signal (e.g. inotify does it), and call finish_wait to update its state and remove itself from the wait queue.\nCompleting the sample code above by managing also the waking up part it will end up with something like this:\n/* ‘q’ is the wait queue we wish to sleep on */ DEFINE_WAIT_QUEUE_HEAD(wait); add_wait_queue(q, \u0026amp;wait); while (!condition) { /* condition is the event that we are waiting for */ prepare_to_wait(\u0026amp;q, \u0026amp;wait, TASK_INTERRUPTIBLE); if (signal_pending(current)) /* handle signal */ schedule(); } finish_wait(\u0026amp;q, \u0026amp;wait); As a detail, wake up can be provoked in both process context and interrupt context, during an interrupt handler execution, which is what often device drivers do. Sleep can be only done in process context.\n 5. Context switch and Preemption And this comes to the context switch. For example, when a task starts to sleep a context switch is needed, and the next task is voluntarily picked and the scheduling is done via schedule().\nThe context switch work is done by context_switch(), called by the internal __schedule() and executes:\n switch_mm() (implementation here for x86) to switch virtual memory mappings process-specific. switch_to() (x86_64) to save and restore stack information and all registers which contain process-specific data.  As you saw, both functions are architecture-dependent (ASM) code. The context switch is requested by the tasks themselves voluntarily or by the scheduler, nonvoluntarily from the point of view of a task.\nVoluntary As we saw tasks can trigger context switch via schedule() in kernel space, either when they explicitly request it or when they put themselves to sleep or they try to wake up other ones. Also, context switch happens when tasks block, for example when synchronizing with semaphores or mutexes.\nAnyway, context switches are not done only when code in kernel space voluntarily calls schedule(), otherwise, tasks could monopolize a CPU, so an external component should intervene.\nNonvoluntary: preemption As the main Linux scheduler class is a fair scheduler the fairness must be guaranteed in some way\u0026hellip; Ok, but how does it preempt?\nFor this purpose, a flag named need_reschedule is present in the task_struct\u0026rsquo;s thread_info flags (i.e. x86) and is set or unset on the current task to notify that it should leave the CPU which in turn, after schedule() call, will switch to another process context.\nSo, when this flag is set?\n in scheduler_tick(), which is constantly called by the timer interrupt handler (the architecture-independent part actually), continuously checking and updating vruntime, balancing the runqueues, it sets the flag when preemption is needed. in try_to_wake_up(), when the current task has minor priority than the awakened.  Then, in order to understand when the flag is checked, we can think about when a task preemption is needed and also can be done safely.\nIn userspace Returning from kernelspace to userspace is safe to context switch: if it is safe to switch mode and continue executing the current task, it is also safe to pick a new task to execute. Has this userspace task still to run? Maybe it’s no longer fair to run it. This is what happens when from:\n system calls interrupt handlers  return to userspace.\nIf need_resched is set a schedule is needed, the next entity task is picked, and context switch done.\n As a note, consider that both these paths are architecture-dependent, and typically implemented in assembly in entry.S (e.g. x86_64) which, aside from kernel entry code, also contains kernel exit code).\n In kernel space A note deserves to be explained. The kernel is fully preemptive from 2.6 that is, a task can be preempted as long as the kernel is in a safe state. When preemption can’t be done, locks are in place to mark it, so that a safe state is defined when the kernel doesn’t hold a lock. Basically, a lock counter preempt_count is added to thread_info flags (x86) to let preempt tasks that are running in kernelspace only when it’s equal to zero.\nUpon return from interrupt to kernelspace or from process context during preemption, if need_resched is set and preempt_count == 0 the current task is preempted, otherwise the interrupt returns to the interrupted task.\nAlso, every time preempt_count is updated and decreased to zero and need_resched is true, preemption is done.\nFor example, considering the return path from interrupt which is architecture-dependent, the xtensa ISA\u0026rsquo;s common exception exit path is pretty self-explanatory:\ncommon_exception_return: ... #ifdef CONFIG_PREEMPTION 6: _bbci.la4, TIF_NEED_RESCHED, 4f /* Check current_thread_info-\u0026gt;preempt_count */ l32ia4, a2, TI_PRE_COUNT bneza4, 4f abi_callpreempt_schedule_irq j4f #endif ...  TL;DR About what we said above, you can check the __schedule() function comments.\n Moreover, the kernel is SMP-safe that is, a task can be safely restored in a symmetrical multi-processor.\nYou can check both preemption config and SMP config (x86) in your running kernel version from procfs:\n$ zcat /proc/config.gz | grep \u0026quot;CONFIG_SMP\\|CONFIG_PREEMPT\u0026quot; | grep -v \u0026quot;^#\u0026quot; CONFIG_PREEMPT_BUILD=y CONFIG_PREEMPT=y CONFIG_PREEMPT_COUNT=y CONFIG_PREEMPTION=y CONFIG_PREEMPT_DYNAMIC=y CONFIG_PREEMPT_RCU=y CONFIG_SMP=y CONFIG_PREEMPT_NOTIFIERS=y That’s all folks! We\u0026rsquo;ve arrived to the end of this little journey.\n Conclusion  The linked code refers to Linux 5.17.9.\n I liked the idea to leave you the choice to dig into each single path the kernel does to manage the tasks scheduling. That\u0026rsquo;s why I intentionally didn\u0026rsquo;t include so many snippets, instead providing you the code face of the coin for almost every path we saw, through links to the real Linux code.\nWhat is incredible is that, even if it\u0026rsquo;s one of the largest OSS projects, you can understand how Linux works and also contribute. That\u0026rsquo;s why I love open source more every day!\nThank you! I hope this was interesting for you as it was for me. Please, feel free to reach out!\n twitter github linkedin  Links  https://www.kernel.org/doc/html/v5.17/scheduler/index.html https://elixir.bootlin.com/linux/v5.17.9/source https://www.amazon.com/Linux-Kernel-Development-Robert-Love/dp/0672329468 https://mechpen.github.io/posts/2020-04-27-cfs-group/index.html#2.2.-data-structures https://josefbacik.github.io/kernel/scheduler/2017/07/14/scheduler-basics.html https://opensource.com/article/19/2/fair-scheduling-linux https://lwn.net/Articles/531853/ https://oska874.gitbooks.io/process-scheduling-in-linux/content/  ","permalink":"https://blog.maxgio.me/posts/linux-scheduler-journey/","summary":"Two years ago more or less I started my journey in Linux. I was scared at first and I didn\u0026rsquo;t know where to start from. But then I decided to buy a book - and what a book! - in order to follow a path.\nAlong the way, I integrated the material with up-to-date documentation from kernel.org and source code. In the meantime, I started to learn C a bit so that I also could have played with what I was learning, step by step.","title":"A journey into the Linux scheduler"},{"content":"Hello everyone, a long time has passed after the 5th part of this journey through STRIDE thread modeling in Kubernetes has been published. If you recall well, STRIDE is a model of threats for identifying security threats, by providing a mnemonic for security threats in six categories:\n Spoofing Tampering Repudiation Information disclosure Denial of service Elevation of privilege  In this last chapter we\u0026rsquo;ll talk about elevation of privilege. Well, this category can be very wide, but let\u0026rsquo;s start thinking about what it can comprises and what we can do against this category of threats.\nElevation of privilege Elevation or escalation of privileges is gaining higher access than what is granted. This in turn can be leveraged to access unauthorized resources or to cause damage. Also, this attack can be conducted through other different types of attacks, like spoofing, where an actor claims to be a different actor with higher privileges, and so on.\nSo the first question we\u0026rsquo;d like to answer could be: what we can generally do? I think we can consider from an high-level point of view prevention and detection: prevention can be done via access control, and detection through analysis of audit events - this assumes we have auditing in place.\nIn Kubernetes Role-Based Access Control authorizes or not access to Kubernetes resources through roles, but we also have underlying infrastructure resources, and Kubernetes provides primitives to authorize workload to access operating system resources, like Linux namespaces.\nAll of this is a bit simplicistic for sure, but just consider this as a starting point for reflections: nothing can be 100% secure, and no solution can exists that can cover all scenarios.\nPrevention Prevention is the act to avoid that an action occurs. In this case we\u0026rsquo;re talking about unwanted actions, like for example that a Pod\u0026rsquo;s container runs with unwanted capabilities like CAP_SYS_ADMIN.\nKubernetes In Kubenetes access control is mostly achieved with usage of roles. The policies that we need are generally specific on the workload that we run, but the recommendation is to follow deny-by-default approach, and to authorize the more minimum set of capabilities as possible. In detail not using default ServiceAccount, configuring and binding proper permissions, and do not mound Service Account token when not needed, is a good choice.\nAt the same time we should consider that also Kubernetes components like kubelet is authorized to access Kubernetes resources like Secrets through Node Authorization. And not only in-cluster authorized requests, but also ones that are authorized externally, like users that are authenticated and authorized by cloud provider/on-premise IAM services through OIDC or SAML flows.\nFurthermore, authorized workload runs in the clusters and sometimes can make API requests to Kubernetes. In the era of GitOps we let further workload to reconcile workload as we\u0026rsquo;d desire. So keep in mind which privileges GitOps controllers need and apply least privileg principle also there. The work that Flux team is doing for modeling their API considering complex scenarios like multi-tenancy is great.\nTalking about access control in multi-tenancy scenarios Capsule is an interesting project which can help with managing access control easily.\nOS We\u0026rsquo;re no longer talking about granting access to Kubenetes resources to Kubernetes workload, instead we\u0026rsquo;re talking about granting access to OS resources to OS workload, as in the end Pods run as tasks at operating system level. Here is where all the magic happens, and where our containers are composed through Linux namespaces, control groups, capabilities.\nI won\u0026rsquo;t go in details of specific container escape techniques like Kamil Potrec did here very well, but I\u0026rsquo;m talking about general approaches and vectors to consider and which prevention we could do. In Kubernetes SecurityContext is what enables or not access to underlying operating system resources to Kubernetes pods.\nAccess control should be in place also at this level, so basically we\u0026rsquo;d want policies to prevent unwanted privileges.\nPolicy design I\u0026rsquo;m talking about policies in general, beyond the implementation. When choosing which level of privileges we\u0026rsquo;d want to allow, let\u0026rsquo;s consider:\n Prevent containers from running as UID 0 in the container user namespace (with securityContext.runAsUser or specifying it from Dockerfile) Drop capabilities: even with unprivileged user namespaces, apply least needed capabilities. See more here about Linux capabilities with user namespaces. Filtering syscalls: as for capabilities, we can use seccomp to filter system calls by using the container\u0026rsquo;s securityContext, for example by blocking common syscalls used in techniques like unshare to create new Linux namespaces or userfaultd to control page faults in userspace after triggering overflows. With this regard, Security Profiles Operator would help to automatically generate initial seccomp or AppArmor profiles, specific to applications. Avoid to attach host namespaces unless strictly needed. I found this repo well done to understand it. Preventing privilege escalation: since in Linux by default a child processes is allowed to claim more privileges than the parent, this is not ideal for containers, so use Pod Security Policy to set allowPrivilegeEscalation: false.  Just a note about unprivileged user namespaces and Kubernetes: starting in Linux 3.8, unprivileged processes can create user namespaces, and the other types of namespaces can be created with just the CAP_SYS_ADMIN capability in the caller\u0026rsquo;s user namespace (Kinvolk explains clearly how it relates to containers here). Rootless containers are based on that, unfortunately Kubernetes doesn\u0026rsquo;t already support them, but Akihiro Suda is pushing effort on his work in progress to have a \u0026ldquo;rootless Kubernetes\u0026rdquo; distribution, which is Usernetes. So, let\u0026rsquo;s try it and give feedbacks!\nPolicy enforcement What can be used to write and enforce policies and so prevent Pods from running with higher privileges with respect to what we would grant to them is Pod Security Policy feature. It provides API objects to declare policies and a validating and also mutating admission controller to enforce them.\nUnfortunately a non-clear path has been drawn in these years letting us with doubts and limits, and leading in the end to deprecation of Pod Security Policy APIs from 1.21. A KEP is in place, that very briefly proposes a simpler approach based on the Pod Security Standards which identifies three basic levels of policies (Privileged, Baseline, Restricted), appliance of them via annotations at Namespace level and enforcement through a new dedicated admission controller.\nKeep it mind that it should cover different scenarios and easy the migration from PSP, but at the same time for more advanced use cases there are different framework like Gatekeeper that allow us to write fine-grained Rego policies with OPA, but also Kyverno that instead doesn\u0026rsquo;t require to learn a new language just to name one. Another option is Polaris, which offers admission controllers that prevents also on this.\nAnyway, always consider that for sure the more powerful the solution the higher the granularity and probability to tailor our scenarios, but also IMHO:\n the more complexity we add, the larger could be the attack surface; the steeper the learning curve, the harder could be the effectiveness to be achieved.  Network Don\u0026rsquo;t forget to think about the network, as also network resources can be used to escalate, such as by getting informations from cloud provider\u0026rsquo;s metadata API. NetworkPolicies enables to do access control at network level. Kinvolk\u0026rsquo;s Inspektor Gadget network-policy gadget could help to generate our Network Policy by inspecting our Pods network activity. Then the Network Policy Editor by Cilium can also teach and generate them.\nDetection Kubernetes Kubernetes provides auditing features through dedicated APIs. Audit Events are events that are recorded by the API server as defined for Policy and sent to backends. From the decision on removal of the dynamic backend feature, there\u0026rsquo;s a proposal on introducing a DynamicAudit Proxy based on static webhook, so this last one remains the fundamental feature to base on.\nOS Auditing at the operating system level can be looked at by inspecting the requests that the containers (and not only) can fire to interact with the OS, so the system calls. Falco is one of the projects that does exactly that, by capturing, inspecting the fired syscalls and filtering the suspictious ones. Alerts can can be shipped to webhook endpoints and with the addition of Falco Sidekick to a lot of backends like object storage services or message queues or chats. Then, also mitigation can be triggered from detection events in Falco, for example with Kubeless.\nFor sure here eBPF plays a fundamental role here as it allow to program the kernel in a safe manner and easily inspect kernel events.\nInspektor Gadget is a collection of tools to do inspection inspired by kubectl-trace plugin which schedules bpftrace programs in Kubernetes clusters. The most relevant gadget for this scope is the traceloop that can help inspecting system calls requested by Pods also in the past. Here what is very interesing is also the capabilities gadget that can help to tailor our Pod container\u0026rsquo;s SecurityContexts. What we\u0026rsquo;d need then is a filtering layer that can fill an alert system for suspictious behaviour.\nKnown vulnerabilities Now that we reason about some possible vectors, let\u0026rsquo;s list known vulnerabilities from which we can defend with detection and prevention.\nCVE-2020-14386 In a couple of words, this can be exploited with kernels before 5.9-rc4. As this privilege escalation work using raw sockets and by default Kubernetes adds CAP_NET_RAW capabilities to the pods, As you may guess, a PodSecurityPolicy that drops this capability can work. But I recommend to dig into it.\nSee here how to detect and mitigate with Falco.\nCVE-2020-8559 The API server in versions from v1.6 to v1.15 and prior to v1.16.13, v1.17.9 and v1.18.6 are vulnerable to an unvalidated re-direct on proxied upgrade requests that could allow an attacker to escalate privileges from a node compromise to a full cluster compromise. So, let\u0026rsquo;s keep Kubernetes up-to-date.\nAlso, we can leverage a tool to hunt on our cluster for weaknesses, which is kube-hunter. You can run it on an external machine or within the cluster on the machine or in a pod.\nConclusion So we talk about what privilege escalation is, which are the resources that we should protect, both when we do prevention and when we do detection.\nAnother conclusion As we go through this journey I learnt a lot of stuff that was new to me. When preparing this part and re-reading the first ones I thought: \u0026ldquo;What is this? Was it me? I should not publish this\u0026rdquo;, and I was going to delete and re-write them. I saw a very different approach, a different consciousness and confusion of what I was talking about.\nBut I thought that this is part of our journey. And I let them published with pride.\nWhat I\u0026rsquo;m tying to say, is that we have another conclusion. No one will know everything and we always are in a continuous journey. We have a lot of value in sharing what we learn and our thoughts. This is why I opened this blog, because I believe in it.\nSo.. Let\u0026rsquo;s keep in touch here, on Twitter, Github or anywhere you want!\n","permalink":"https://blog.maxgio.me/posts/stride-threat-modeling-kubernetes-elevation-of-privileges/","summary":"Hello everyone, a long time has passed after the 5th part of this journey through STRIDE thread modeling in Kubernetes has been published. If you recall well, STRIDE is a model of threats for identifying security threats, by providing a mnemonic for security threats in six categories:\n Spoofing Tampering Repudiation Information disclosure Denial of service Elevation of privilege  In this last chapter we\u0026rsquo;ll talk about elevation of privilege. Well, this category can be very wide, but let\u0026rsquo;s start thinking about what it can comprises and what we can do against this category of threats.","title":"STRIDE threat modeling on Kubernetes pt.6/6: Elevation of privilege"},{"content":"I\u0026rsquo;m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes. In the previous one we talked about Information disclosure. This part is about the D that stands for Denial Of Service.\nDOS is the attempt to making a resource unavailable. For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company\u0026rsquo;s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (really happened - thanks Peter).\nTherefore, an induced lack of resources is what generally leads to unavailability.\nSo, how we can do prevention? We can do it with:\n Increased Availability Resource isolation Resource monitoring Moreover, vulnerability-specific patches  Now let\u0026rsquo;s jump into Kubernetes world and think about splitting up the different layers on which to guarantee availability: Nodes Network Control plane Workload\nAs the availability can be increased on all resources, I\u0026rsquo;ll sum up briefly what we can do.\nMaster nodes  Deploy multiple master nodes to provide HA on the control plane (for instance to protect from direct attacks to the API server); Deploy on multiple datacenters (to protect from attacks on the network to a particular datacenter).  Worker nodes   Deploy on multiple datacenters (to protect from attacks on the network to a particular datacenter);\n  Configure resource limits per namespace by using ResourceQuotas for:\n CPU and memory; Storage (PVC per StorageClass); Object count; Extended resources (only limit);    Configure resource limits per container;\n can also be useful for scheduling purposes with Pod Priority, and can be able to define the workload\u0026rsquo;s Quality of Service; use LimitRanges to set resource defaults;    Configure out of resource handling to reclaim resources by notifying under pressure nodes to the kubelet;\n  Configure Cluster Autoscaler to gain availability based on your workload.\n  Workload  Configure Horizontal Pod Autoscaler; Configure correct resources limits other than requests; Configure Vertical Pod Autoscaler or addon-resizer; you can also leverage the VPA in Off mode in order to get only recommendations for setting appropriate resources for your workload; Define Pod-to-Pod and Pod-to-external Network Policies; Configure mutual TLS and proper API authentication mechanism.  API server  Configure high availability; Configure monitoring and alerting on requests and Audit; Isolate: do not expose the endpoint on Internet, for instance syn flood attacks could be in place.  etcd  Configure HA; Configure monitoring and alerting on requests; Isolate: so that only the control plane members can access it; As a plus, configure dedicated cluster, since etcd is one of the main bottlenecks and to provide resilience from the other control plane components (e.g. if they are compromised).  Network  Configure rate limiting at Ingress Controller level to limit connections and requests per seconds/minute per IP (for example with NGINX ingress controller); Deny source IPs with Network policies.  Then, other than following all the best practices there could also be vulnerabilities on components that we generally consider already secured; so let\u0026rsquo;s sum up a couple of them.\nKnown vulnerabilities CVE-2019–9512: Ping Flood with HTTP/2 The attacker hammers the HTTP/2 listener with a continuous flow of ping requests. To respond, the recipient start queuing the responses, leading to growing queues and then allocating more memory and CPU.\nCVE-2019–9514: Reset Flood with HTTP/2 The attacker can open several streams to the server and sending invalid data through them. Having received invalid data, the server sends HTTP/2 RST_STREAM frames to the attacker to cancel the \u0026ldquo;invalid\u0026rdquo; connection.\nWith lots of RST_STREAM responses, they start to queue. As the queue gets more massive, more and more CPU and memory get allocated to the application until it eventually crashes.\nKubernetes has released the required patches to mitigate the issues as mentioned above. The new versions were built using the patched versions of Go so that the required fixed are applied to the net/http library.\nFixed versions:\n Kubernetes v1.15.3 - go1.12.9 Kubernetes v1.14.6 - go1.12. Kubernetes v1.13.10 - go1.11.13  CVE-2020–8557: Node disk DOS The /etc/hosts file mounted in a pod by kubelet is not included by the kubelet eviction manager when calculating ephemeral storage usage by a pod. If a pod writes a large amount of data to the /etc/hosts file, it could fill the storage space of the node. Affected versions:\n kubelet v1.18.0–1.18.5 kubelet v1.17.0–1.17.8 kubelet \u0026lt; v1.16.13  Fixed Versions:\n kubelet master - fixed by #92916 kubelet v1.18.6 - fixed by #92921 kubelet v1.17.9 - fixed by #92923 kubelet v1.16.13 - fixed by #92924  Prior to upgrading, this vulnerability can be mitigated by using PodSecurityPolicies or other admission webhooks to force containers to drop CAP_DAC_OVERRIDE or to prohibit privilege escalation and running as root. Consider anyway that these measures may break existing workloads that rely upon these privileges to function properly.\nCVE-2020–8551: Kubelet DoS via API The kubelet has been found to be vulnerable to a denial of service attack via kubelet API, including the unauthenticated HTTP read-only API typically served on port 10255, and the authenticated HTTPS API typically served on port 10250.\nAffected Versions:\n kubelet v1.17.0 - v1.17.2 kubelet v1.16.0 - v1.16.6 kubelet v1.15.0 - v1.15.9  Fixed Versions\n kubelet v1.17.3 kubelet v1.16.7 kubelet v1.15.10  In order to mitigate this issue limit access to the kubelet API or patch the kubelet.\nCVE-2020–8552: Kubernetes API Server OOM The API server has been found to be vulnerable to a denial of service attack via authorized API requests.\nAffected Versions:\n kube-apiserver v1.17.0 - v1.17.2 kube-apiserver v1.16.0 - v1.16.6 kube-apiserver \u0026lt; v1.15.10  Fixed Versions:\n kube-apiserver v1.17.3 kube-apiserver v1.16.7 kube-apiserver v1.15.10  Prior to upgrading, this vulnerability can be mitigated by preventing unauthenticated or unauthorized access to all apis and by ensuring that the API server automatically restarts if it OOMs.\nCVE-2019–1002100: Kubernetes API Server JSON-patch parsing Users that are authorized to make patch requests to the Kubernetes API server can send a specially crafted patch of type json-patch (e.g. kubectl patch - type json or Content-Type: application/json-patch+json) that consumes excessive resources while processing, causing a denial of service on the API server.\nAffected versions:\n Kubernetes v1.0.x-1.10.x Kubernetes v1.11.0–1.11.7 Kubernetes v1.12.0–1.12.5 Kubernetes v1.13.0–1.13.3  Fixed Versions:\n Kubernetes v1.11.8 Kubernetes v1.12.6 Kubernetes v1.13.4  Prior to upgrading, this vulnerability can be mitigated by removing patch permissions from untrusted users.\nCVE-2019–11253: Kubernetes API Server JSON/YAML parsing This is a vulnerability in the API server, allowing authorized users sending malicious YAML or JSON payloads to cause kube-apiserver to consume excessive CPU or memory, potentially crashing and becoming unavailable.\nPrior to v1.14.0, default RBAC policy authorized anonymous users to submit requests that could trigger this vulnerability.\nClusters upgraded from a version prior to v1.14.0 keep the more permissive policy by default for backwards compatibility. Here you can find the more restrictive RBAC rules that can mitigate the issue.\nAffected versions:\n Kubernetes v1.0.0–1.12.x Kubernetes v1.13.0–1.13.11 Kubernetes v1.14.0–1.14.7 Kubernetes v1.15.0–1.15.4 Kubernetes v1.16.0–1.16.1  Fixed Versions:\n Kubernetes v1.13.12 Kubernetes v1.14.8 Kubernetes v1.15.5 Kubernetes v1.16.2  Consider that if you are running a version prior to v1.14.0, in addition to installing the restrictive policy, turn off autoupdate for the applied ClusterRoleBinding so your changes aren\u0026rsquo;t replaced on an API server restart.\nOn the related Github issue you can find more details that I didn\u0026rsquo;t insert here for conciseness.\nConclusion So, that\u0026rsquo;s all folks! If we followed all these rules and applied the released patches that\u0026rsquo;s a good starting point for prevention and can also help on detection and remediation.\nStay tuned for the next and final episode about the E of STRIDE: Escalation of privileges!\n","permalink":"https://blog.maxgio.me/posts/k8s-stride-05-denial-of-service/","summary":"I\u0026rsquo;m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes. In the previous one we talked about Information disclosure. This part is about the D that stands for Denial Of Service.\nDOS is the attempt to making a resource unavailable. For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company\u0026rsquo;s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (really happened - thanks Peter).","title":"STRIDE threat modeling on Kubernetes pt.5/6: Denial of service"},{"content":"This is the fourth part of a series about STRIDE threat modeling in Kubernetes. In the previous part we talked about repudiation, instead today we\u0026rsquo;ll going to address information disclosure.\nInformation disclosure happens with data leaks or data breaches, whenever a system that is designed to be closed to an eavesdropper unintentionally reveals some information to unauthorized parties.\nTo prevent this we should protect data in transit and at rest by guaranteeing confidentiality, which can be guaranteed with encryption.\nMost of the data in Kubernetes is part of his state, the sensitive part of which is represented by Secret API objects, stored in ectd. So, first of all, to prevent information disclosure we should encrypt data, especially secret, at rest and in transit.\nFurthermore, we should consider also data that can be sensitive or at least be used to gain extra privileges in the cluster or in the cloud provider, such as all sensitive data generated by our workload that won\u0026rsquo;t be stored in Secrets or, for example, cloud metadata from the cloud provider\u0026rsquo;s API.\nBut let\u0026rsquo;s go deeper and start from data at rest.\nData at rest As we said, to avoid unwanted parts access sensitive data, we can restrict access to it. At the same time, to avoid that even if that data is unintentionally accessed it can be read, we can encrypt it.\nRestrict access etcd Only the API server and other etcd nodes should be able to access etcd since write access is equivalent to gaining root on the entire cluster, and read access can be used to gain extra privileges.\nYou can enforce restrictions on it with ACL at firewall-level and with strong authentication with PKI and X.509 certificates, which are supported by etcd.\nTwo are the authentication to consider:\n between etcd peers between the API server and etcd.  To secure communications between etcd peers you can use these etcd flags:\n –peer-key-file=\u0026lt;peer.key\u0026gt; –peer-cert-file=\u0026lt;peer.cert\u0026gt; –peer-client-cert-auth –peer-trusted-ca-file=\u0026lt;etcd-ca.cert\u0026gt;  To secure communication between the API server and etcd use these ones:\n –cert-file=\u0026lt;server.crt\u0026gt; –key-file=\u0026lt;server.key\u0026gt; –client-cert-auth –trusted-ca-file=\u0026lt;etcd-ca.crt\u0026gt;  and at the same time configure the API server with these flags:\n –etcd-certfile=\u0026lt;client.crt\u0026gt; –etcd-keyfile=\u0026lt;client.key\u0026gt;  Note: in this case in order to allow API server to communicate with etcd the certificate client.crt should be trusted by the CA with certificate etcd-ca.crt. Here you can find scripts to setup certs and key pairs for these individuals communications.\nConsider all of that for learning purposes because if you - or your provider - set up the cluster with kubeadm, it manages all of that by built-in and you likely won\u0026rsquo;t need to setup PKI for etcd and API server by yourself, except only for advanced cases.\nAnyway, we\u0026rsquo;ll talk about PKI management with kubeadm later.\nSecret API When deploying applications that interact with the Secret API, you should limit access using RBAC policies.\nConsider that Secrets can also be used to gain extra privileges, as for service account tokens. For example, components that create pods inside system namespaces like kube-system can be unexpectedly powerful because those pods can gain access to service account secrets or run with elevated permissions if those service accounts are granted access to permissive PSPs.\nFor this reason always review permissions that components need: generally watch and list requests for secrets within a namespace should be avoided since listing Secrets allows the clients to inspect the values of all Secrets that are in that namespace. The permission to watch and list all Secrets in a cluster should be reserved only to trusted system components. To let applications access the Secret objects it should be allowed only get requests on the needed Secrets, in order to apply the least privilege principle and denying by default.\nFurthermore, consider that if a user can create a Pod that uses a Secret, he could expose the Secret even if the API server policy does not allow that user to read the Secret.\nAnother method to access the Secret API objects can be by impersonating the kubelet because it can be read any Secret from the API server. You can prevent this by enforcing authentication and authorization restrictions on the kubelet binary, on the RBAC permissions, but in this case, you should migrate to manage the authorization on kubelets with Node Authorization and enabling the Node restriction admission controller.\nIn addition, review the RBAC permissions of the objects that access the kubelet.\nSecret Volumes A Secret is sent to a node only if a Pod on that node requires it and the kubelet stores the Secret into a tmpfs so that the Secret is not written to disk storage. Once the Pod that depends on the Secret is deleted, the kubelet will delete its local copy of the secret data as well.\nFurthermore, only the Secrets that a pod requests are potentially visible within its containers. Therefore, one Pod does not have access to the Secrets of another Pod. And each container in a Pod has to request the Secret volume in its volumeMounts for it to be visible within the container.\nSo, the prevention can be made by design, by separating the responsibilities and by not exposing services that have access to Secrets as much as possible.\nYou can enforce Secret access control from Pods with external secret managers like Vault that from December 2019 supports Secret injection with vault-k8s, which is a Kubernetes mutating admission control webhook that alters pod spec to include Vault Agent containers that render Vault secrets to a shared memory volume, so that containers within the pod only need to read filesystem data without being aware of Vault.\nHere you can read their blog post about it.\nCloud metadata API In addition to data restrictively stored in Kubernetes, cloud providers often expose metadata services locally to instances. By default, these APIs are accessible by pods running on an instance and can contain cloud credentials for that node, or provisioning data such as kubelet credentials. These credentials can be used to escalate within the cluster or to other cloud services under the same account.\nWhen running Kubernetes on a cloud platform limit permissions are given to instance identities (for example fine-tuning IAM instance roles in AWS), use Kubernetes Network Policies to restrict pod access to the metadata API, and avoid using provisioning data to deliver secrets.\nEncrypt etcd You can encrypt data stored in etcd by enable Kubernetes encryption at rest feature, that is available in beta version from version 1.13, so that the Secrets are not stored in plaintext into etcd, and even if an attacker can gain access to it, he can\u0026rsquo;t read it; so let\u0026rsquo;s see briefly how it works.\nThe API server binary accepts an argument - encryption-provider-config to pass an encryption configuration that controls how data is encrypted in etcd. The configuration is an API object that is part of the apiserver.config.k8s.io API group, as you can see in the example below:\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- aescbc:keys:- name:key1secret:\u0026lt;BASE 64 ENCODED SECRET\u0026gt;- identity:{}As you can see each resources array item is a separate complete configuration; the resources.resources field is an array of resource names (i.e. secrets) that should be encrypted.\nThe providers array is an ordered list of the possible encryption providers which currently are:\n identity aescbc secretbox aesgcm kms  Important: note that as the first provider specified in the list is used to encrypt resources, and since the identity provider is used by default which provides no encryption, if you place it as the first item you disable encryption.\nFurthermore, since the config file can contain keys that can decrypt content in etcd, you should restrict permissions on your master nodes so only the user who runs the API server can read it.\nA better approach is envelope encryption where it\u0026rsquo;s generated a data encryption key that encrypts the data, and then the data encryption key is encrypted with a key encryption key to protect it. It could be encrypted also the KEK but eventually, one key must be in plaintext in order to decrypt the keys and finally the data. The top-level key is the master key and it should be stored outside of the cluster.\nDoing so, you don\u0026rsquo;t have to worry about storing the encrypted data key, because it is protected by encryption; furthermore, since the data could be large, you gain in performance as you don\u0026rsquo;t have to re-encrypt multiple times the same data with different keys, but you can re-encrypt only the data keys that protects the data.\nThe KMS encryption provider uses envelope encryption to encrypt data in etcd and the master/key encryption key is stored and managed in a remote KMS, letting the user be able to rotate it. The KMS provider uses gRPC to communicate with a specific KMS plugin; the KMS plugin, which is implemented as a gRPC server, communicates with the remote KMS.\nNote that if you are using EKS they just introduced the support for envelope encryption of Secrets with KMS a little time ago; here the announcement.\nOther than etcd, you can prevent from reading secret data by encrypting your backups as well as full disks; for example, if your cluster runs in AWS on EC2 instances you can enable encryption of EBS volumes, or if you use EKS for example is provided by default.\nVCS If you configure secret data in general (Secrets objects, inputs for Kustomize secret generators, chart values, etc.) through a manifest which has the secret data encoded as base64 and you choose to put under git versioning that data, it means the secret is compromised becase Base64 encoding is not an encryption method and it will be the same as plain text.\nYou can protect that versioned secret by encrypting it using tools like git-crypt or even better SOPS, which integrates very well with most cloud provider\u0026rsquo;s KMS, other than PGP.\nYou can reach a level of granularity where you use an AWS KMS Customer Master Key and a least-privilege AWS IAM role both dedicated only to a single file inside a repository, to encrypt and decrypt… that\u0026rsquo;s amazing! If you are interested and you don\u0026rsquo;t already know it, this introductive video is recommended.\nFurthermore, as Helm is the de-facto standard for package management and you likely want to version chart configurations, in case these contain secret data you likely also want to protect them. Even here you can leverage SOPS by using the helm-secrets Helm plugin. Logs\nA little additional point: even if all the above measures taken place, be sure to not accidentally expose secret data read from volumes or environment by writing it to logs or shipping it to external services, for example data collectors.\nData in transit Encrypt Other than the data at rest is important to encrypt in-transit data. TLS provides a protocol to manage encryption of the data in communications between two parties.\nIt defines how they agree on the cipher suite, that is a set of algorithms to secure the connection, which usually contains the algorithm used to exchange the encryption key, the encryption algorithm itself, the message authentication code (MAC) algorithm for integrity check, and optionally an authentication algorithm. TLS itself decides which are the required features of supported cipher suites (for example in TLS 1.3, many legacy algorithms have been dropped).\nAuthentication in TLS is an optional feature but is highly recommended, and by default using the security features of etcd you protect in-transit data with encryption via TLS and restrict the communication to only trusted peers with a Public Key Infrastructure and X.509 certificates.\nIn the Kubernetes world in order to protect the communications you must consider data transmitted between and to Kubernetes components and between application components.\nBetween/to Kubernetes components On most Kubernetes distributions, communication between master and worker components, is protected by TLS. Specifically the same applies between etcd nodes, and between API server and etcd as explained above.\nIn this way data and specifically sensitive data in transit is protected when transmitted over these channels.\nIf you install Kubernetes with kubeadm, the certificates that your cluster requires are automatically generated and placed under /etc/kubernetes/pki. Consider that Kubernetes requires PKI for the following operations:\n Client certificates for the kubelet + kubeconfig to authenticate to the API server Server certificate for the API server endpoint Client certificates + kubeconfig for administrators of the cluster to authenticate to the API server Client certificates for the API server to talk to the kubelets Client certificate for the API server to talk to etcd Client certificate + kubeconfig for the controller manager to talk to the API server Client certificate + kubeconfig for the scheduler to talk to the API server Client and server certificates for etcd to authenticate between themselves  If you run kube-proxy to support an extension API Server, client and server certificates for the front-proxy\nIn case you don\u0026rsquo;t want to let kubeadm to create these certificates for example because you need to integrate your certificate infrastructure into a kubeadm-built cluster you can either create and inject intermediate CAs from your root CA to let kubeadm to create the certificates or, in case you don\u0026rsquo;t want to copy your CAs into the cluster you can create all the certificates by yourself. Anyway proceed only if you know what are you doing and consider that in most cases the default kubeadm configuration is fine.\nFor the control plane, the certificates are valid for one year and kubeadm renews them by default during the control plane upgrade. If this configuration does not fit your need you can disable renewal during upgrade by passing - certificate-renewal=false option to kubeadm upgrade apply or to kubeadm upgrade node.\nThen, you can even manage the renewals manually via the Kubernetes Certificates API by signing the certificates with the controller manager\u0026rsquo;s built-in signer or by using systems like cert-manager; furthermore, you can also renew certificates with your external CA and let kubeadm create only your CSRs.\nInstead, the certificate and kubeconfig of the kubelet are automatically updated by themselves; take a look here also for automatic certificates bootstrapping of the kubelets for communications to the API server, needed for example when scaling up the worker nodes.\nBy the way, you likely won\u0026rsquo;t need to do a lot of work to secure communications between Kubernetes components, as kubeadm manages most of the parts of the PKI; moreover, if a cloud provider hosts your cluster it probably also offers additional operational features.\nBetween application components For components that you develop and deploy, consider to encrypt the communications with mTLS to provide mutual authentication between them, by leveraging service meshes like Istio or Linkerd, where other than encryption you guarantee that both components are trusting each other and they manage entirely the PKI for your mesh.\nYou could also setup and manage PKI by yourself by using certificate managers like cert-manager that works with external CAs, for example,b provided by Vault (yes, it can also be used as a root/intermediate CA). Anyway, it\u0026rsquo;s not a good idea since it can be complex, especially in situations where you have microservices… you would freak out.\nA service mesh framework provides also a lot of features; on the security perspective, it can covers also authorization and audit, other than authentication (and encryption), as Istio does with secure naming that maps server identities encoded in certificates with service names and checks against policies, to control if an identity is authorized to run a service.\nBut that is only a part of the security aspects that a service mesh covers; it can manage load balancing, access control, observability, canary releasing, etc. They can cover also the authentication from the end-user, other than in communications between services, for example by supporting OpenID Connect providers. In addition to the encryption of the whole communication, you can and should also encrypt specific sensitive data by leveraging secrets manager like Vault; it also provides encryption as a service thanks to its transit secrets engine.\nYou can use Vault to generate and manage tokens, which in turn - as said before - can be injected runtime into your workload without being aware of the secrets manager.\nFinally, if you\u0026rsquo;d deploy third-party components consider that Kubernetes expects that all API communication in the cluster is encrypted by default with TLS, and the majority of installation methods will allow the necessary certificates to be created and distributed to the cluster components.\nAnyway some components and installation methods may enable local ports over HTTP, so you should check every setting to identify potentially unsecured traffic and, if supported, enable TLS encryption and if not, look for alternative components that will do.\nConclusion \u0026ldquo;The power of a system comes more from the relationships among programs than from the programs themselves\u0026rdquo; (The UNIX Programming Environment). As the UNIX way probably shaped the Kubernetes architecture the relationships are to be carefully managed and secured as like as the data that transits in and be generated by.\nWhereas this, this part covered a lot of aspects and because of that it has been hard to put practice demos; anyway I tried to provide as much references as I can to let you deepen by yourself the topics you\u0026rsquo;re most interested in.\nThat\u0026rsquo;s all folks! I hope you liked this part and to I hope to see you later in the next part about Denial of service.\nHappy hacking!\n","permalink":"https://blog.maxgio.me/posts/k8s-stride-04-information-disclosure/","summary":"This is the fourth part of a series about STRIDE threat modeling in Kubernetes. In the previous part we talked about repudiation, instead today we\u0026rsquo;ll going to address information disclosure.\nInformation disclosure happens with data leaks or data breaches, whenever a system that is designed to be closed to an eavesdropper unintentionally reveals some information to unauthorized parties.\nTo prevent this we should protect data in transit and at rest by guaranteeing confidentiality, which can be guaranteed with encryption.","title":"STRIDE threat modeling on Kubernetes pt.4/6: Information disclosure"},{"content":"Hi all, this is the third part of this little series about STRIDE threat modeling on Kubernetes. Previously we talked about Tampering; today we talk about Repudiation.\nRepudiation is the ability to cast doubt on something that happened. What typically happens is that the attacker aims to deny the authorship of his actions.\nGenerally the opposite and thus the desired goal is prooving:\n What When Where Why Who How  on certain actions. Non-repudiation refers to a situation where a statement\u0026rsquo;s author cannot successfully dispute its authorship and involves associating actions or changes with a unique individual.\nSo, we can mitigate the risks by enabling auditing on the Kubernetes components and gain visibility on actions performed by individual users, administrators or components of the system.\nLet\u0026rsquo;s split out the components into two categories:\n Kubernetes components Underline components  Kubernetes components API Server As the focal point to the API and the front end of the control plane, kube-apiserver performs auditing on the requests and for each of them, it generates an event.\nEach event is then pre-processed according to a policy and then written to a backend.\nThe policy determines what\u0026rsquo;s recorded and the backend persists the records, which can be log files or webhooks.\nEach request can be recorded with an associated stage, which are:\n RequestReceived - The stage for events generated as soon as the audit handler receives the request, and before it is delegated down the handler chain. ResponseStarted - Once the response headers are sent, but before the response body is sent. This stage is only generated for long-running requests (e.g. watch). ResponseComplete - The response body has been completed and no more bytes will be sent. Panic - Events generated when a panic occurred.  Audit Policy When an event is processed, it\u0026rsquo;s compared against the list of rules of the Audit Policy in order. The first matching rule sets the audit level of the event.\nIn order to enable a policy, you can pass the policy file to the kube-apiserver command using the - audit-policy-file flag.\nIs important to note that configuring a correct policy is crucial, so when configuring your own audit policy is recommended to refer to the GCE policy:\napiVersion:audit.k8s.io/v1kind:Policyrules:# The following requests were manually identified as high-volume and low-risk,# so drop them.- level:Noneusers:[\u0026#34;system:kube-proxy\u0026#34;]verbs:[\u0026#34;watch\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;endpoints\u0026#34;,\u0026#34;services\u0026#34;,\u0026#34;services/status\u0026#34;]- level:None# Ingress controller reads \u0026#39;configmaps/ingress-uid\u0026#39; through the unsecured port.# TODO(#46983): Change this to the ingress controller service account.users:[\u0026#34;system:unsecured\u0026#34;]namespaces:[\u0026#34;kube-system\u0026#34;]verbs:[\u0026#34;get\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;configmaps\u0026#34;]- level:Noneusers:[\u0026#34;kubelet\u0026#34;]# legacy kubelet identityverbs:[\u0026#34;get\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;nodes\u0026#34;,\u0026#34;nodes/status\u0026#34;]- level:NoneuserGroups:[\u0026#34;system:nodes\u0026#34;]verbs:[\u0026#34;get\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;nodes\u0026#34;,\u0026#34;nodes/status\u0026#34;]- level:Noneusers:- system:kube-controller-manager- system:kube-scheduler- system:serviceaccount:kube-system:endpoint-controllerverbs:[\u0026#34;get\u0026#34;,\u0026#34;update\u0026#34;]namespaces:[\u0026#34;kube-system\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;endpoints\u0026#34;]- level:Noneusers:[\u0026#34;system:apiserver\u0026#34;]verbs:[\u0026#34;get\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;namespaces\u0026#34;,\u0026#34;namespaces/status\u0026#34;,\u0026#34;namespaces/finalize\u0026#34;]- level:Noneusers:[\u0026#34;cluster-autoscaler\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;update\u0026#34;]namespaces:[\u0026#34;kube-system\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;configmaps\u0026#34;,\u0026#34;endpoints\u0026#34;]# Don\u0026#39;t log HPA fetching metrics.- level:Noneusers:- system:kube-controller-managerverbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;]resources:- group:\u0026#34;metrics.k8s.io\u0026#34;# Don\u0026#39;t log these read-only URLs.- level:NonenonResourceURLs:- /healthz*- /version- /swagger*# Don\u0026#39;t log events requests.- level:Noneresources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;events\u0026#34;]# node and pod status calls from nodes are high-volume and can be large, don\u0026#39;t log responses for expected updates from nodes- level:Requestusers:[\u0026#34;kubelet\u0026#34;,\u0026#34;system:node-problem-detector\u0026#34;,\u0026#34;system:serviceaccount:kube-system:node-problem-detector\u0026#34;]verbs:[\u0026#34;update\u0026#34;,\u0026#34;patch\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;nodes/status\u0026#34;,\u0026#34;pods/status\u0026#34;]omitStages:- \u0026#34;RequestReceived\u0026#34;- level:RequestuserGroups:[\u0026#34;system:nodes\u0026#34;]verbs:[\u0026#34;update\u0026#34;,\u0026#34;patch\u0026#34;]resources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;nodes/status\u0026#34;,\u0026#34;pods/status\u0026#34;]omitStages:- \u0026#34;RequestReceived\u0026#34;# deletecollection calls can be large, don\u0026#39;t log responses for expected namespace deletions- level:Requestusers:[\u0026#34;system:serviceaccount:kube-system:namespace-controller\u0026#34;]verbs:[\u0026#34;deletecollection\u0026#34;]omitStages:- \u0026#34;RequestReceived\u0026#34;# Secrets, ConfigMaps, and TokenReviews can contain sensitive \u0026amp; binary data,# so only log at the Metadata level.- level:Metadataresources:- group:\u0026#34;\u0026#34;# coreresources:[\u0026#34;secrets\u0026#34;,\u0026#34;configmaps\u0026#34;]- group:authentication.k8s.ioresources:[\u0026#34;tokenreviews\u0026#34;]omitStages:- \u0026#34;RequestReceived\u0026#34;# Get repsonses can be large; skip them.- level:Requestverbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]resources:${known_apis}omitStages:- \u0026#34;RequestReceived\u0026#34;# Default level for known APIs- level:RequestResponseresources:${known_apis}omitStages:- \u0026#34;RequestReceived\u0026#34;# Default level for all other requests.- level:MetadataomitStages:- \u0026#34;RequestReceived\u0026#34;Note also that the Audit Policy stands in the audit.k8s.io API group and the current version is v1.\nAudit Backend Audit backends persist audit events to an external storage. Kube-apiserver out of the box provides three backends:\n Log backend, which writes audit events to a file in JSON format; Webhook backend, which sends audit events to a remote API, which is assumed to be the same API as kube-apiserver exposes; Dynamic backend, which configures webhook backends through an AuditSink API object.  Both logging and webhook backend support batching (enabled by default in webhook and disabled in log), for example to buffer events and asynchronously process them (In this case take tuning into account); they support also truncation.\nOther Kubernetes components Generally, the kubelet and container runtime write logs to journald, on machines with systemd. If systemd is not present, they write to .log files in the /var/log directory. System components inside containers like the kube-scheduler and the kube-proxy always write to the /var/log directory, bypassing the default logging mechanism.\nUnderline components Once covered the high level of the stack is important to audit also the underline components, from the container logs to the syscalls.\nContainer logs Generally speaking, in order to decouple at scale the logging system from the application a standard interface to log streams should be used, and what\u0026rsquo;s more standard of the standard streams?\nI recommended writing container logs to stdout and stderr also because is handled and redirected somewhere by the container engine. For example, the Docker container engine redirects those two streams to a logging driver, which is configured in Kubernetes to write to a file in json form.\nSyscalls Instead, speaking of system calls Falco can detect and alert on any behavior that involves making Linux system calls. For example, you can easily detect when:\n A shell is run inside a container A server process spawns a child process of an unexpected type A sensitive file, like /etc/shadow, is unexpectedly read A non-device file is written to /dev A standard system binary (like ls) makes an outbound network connection  Falco is deployed as a long-running daemon and is configured via a general configuration file and a rules file that is meant to be tailored to needs. Here you can see example rules that can detect anomalous events.\nWhen Falco detects suspicious behavior, it sends alerts via one or more channels:\n Writing to standard error Writing to a file Writing to syslog Pipe to a spawned program. A common use of this output type would be to send an email for every Falco notification.  One difference between Falco and other tools is that Falco runs in userspace, using a kernel module and eBPF probes to obtain system calls and bring them to userspace, while the other tools perform system call filtering/monitoring at the kernel level; thanks to that it can have a much richer set of information powering its policies.\nBeyond system calls Falco\u0026rsquo;s event sources can be also Kubernetes Events that are filtered through these rules. For this purpose, it exposes a webhook endpoint that can be used as a Kubernetes Audit webhook backend.\nIn order to install and configure it please refer to the official docs.\nLogs management As for all logs, it is important to collect them and possibly ship them to a centralized and secured log store, available for further process.\nDifferent shipping and collecting tools can be leveraged, such as fluentd and logstash. We\u0026rsquo;ll not deepen about it here for conciseness.\nConclusion This post is not intended to provide the truth, instead to provide insights from my point of view. I purposely did not cover the auditing implementations of the cloud providers, to shift the focus on the fundamentals.\nI hope it was interesting for you, if you liked it please let me know, if you don\u0026rsquo;t agree please let me know, I appreciate sharing opinions and I always aim to learn something new and from different points of view!\nFor this post that\u0026rsquo;s all, happy hacking!\n","permalink":"https://blog.maxgio.me/posts/k8s-stride-03-repudiation/","summary":"Hi all, this is the third part of this little series about STRIDE threat modeling on Kubernetes. Previously we talked about Tampering; today we talk about Repudiation.\nRepudiation is the ability to cast doubt on something that happened. What typically happens is that the attacker aims to deny the authorship of his actions.\nGenerally the opposite and thus the desired goal is prooving:\n What When Where Why Who How  on certain actions.","title":"STRIDE threat modeling on Kubernetes pt.3/6: Repudiation"},{"content":"In the previous post of this little series we talked about preventing spoofing on Kubernetes. Today we\u0026rsquo;ll talk about the T of STRIDE: Tampering.\nTampering is the act of changing something in a malicious way, to gain extra privileges or for denial of service.\nGenerally for preventing tampering is important to:\n limit the access to critical components; control the access to critical components;  Furthermore, it\u0026rsquo;s important to watch for evidence of tampering.\nGenerally, a common solution to highlight instances of tampering could be using seals, but let\u0026rsquo;s apply these concepts to the Kubernetes world.\nLimit access Control plane To protect data at rest, restrict access to master nodes to protect data on etcd. Furthermore, encrypt etcd data, especially Secrets.\nTo protect data in transit, TLS guarantees privacy besides integrity. The communication from the cluster to the API Server is TLS-encrypted (see here to secure traffic to the API Server).\nData plane From the developer perspective - even if the line is not so clear - set containers root filesystem to read-only using SecurityContexts, at container or pod level, since the data that needs to be written is usually persisted through volumes:\napiVersion:v1kind:Podmetadata:name:my-secure-podspec:containers:# ...securityContext:readOnlyRootFilesystem:true# ...From the administrator perspective, the best defense against data tampering is to validate data before processing it. For example the vulnerability CVE-2019–11253 was found last year and this is the related issue on GitHub, there are also described recommended mitigation actions.\nYou can validate pods through Pod Security Policies. They are implemented as an additional Admission Controller and you can prevent for:\n creating Pods with non read only root filesystems: apiVersion:policy/v1beta1kind:PodSecurityPolicymetadata:name:my-psp-ro-rootfs# ...spec:# ...readOnlyRootFilesystem:false# ... creating Pods that access host filesystem to not allowed paths through hostPath volumes, by specifying a whitelist of host paths that are allowed to be used by hostPath volumes: apiVersion:policy/v1beta1kind:PodSecurityPolicymetadata:name:my-psp-hostpaths# ...spec:# ...allowedHostPaths:- pathPrefix:\u0026#34;/example\u0026#34;readOnly:true# ...  Important: Remember to authorize the policies before enabling the Pod Security Policy admission controller, otherwise it will prevent any pods from being created in the cluster. See here to know how to do it.\nOther than PSP you can configure Open Policy Agent that is an open source, general-purpose policy engine that unifies policy enforcement; OPA GateKeeper integrates OPA with Kubernetes.\nApplication Restrict access to the container images' registry. For example, on AWS you can enforce IAM policies on ECR repositories.\nConfiguration Restrict access to the repositories of the configuration files. It can be obvious but do not store sensitive data on repositories, instead use Secrets. Moreover, evaluate if you need a secrets manager such as Vault; you can use it to inject Secrets through sidecars.\nControl access Enable auditing on Kubernetes binaries. Furthermore, you can leverage additional security solutions like Falco, an eBPF-powered OSS for cloud native runtime security that is now part of the CNCF.\nI recommend to see this session to see how it can capture potentially abnormal system events through its set of rules and send them to its audit endpoint through Kubernetes Webhooks.\nIt can be installed standalone or as a DaemonSet; follow this guide to know how to install and use it.\nWatch for evidence of tampering Verify downloaded binaries for the container runtime by running SHA-2 checksum. For the purpose of the conciseness of this post I don\u0026rsquo;t talk about it here, but you can read this simple howto.\nThat\u0026rsquo;s all for this part, thank you and stay tuned for the next one.\nHappy hacking!\n","permalink":"https://blog.maxgio.me/posts/k8s-stride-02-tampering/","summary":"In the previous post of this little series we talked about preventing spoofing on Kubernetes. Today we\u0026rsquo;ll talk about the T of STRIDE: Tampering.\nTampering is the act of changing something in a malicious way, to gain extra privileges or for denial of service.\nGenerally for preventing tampering is important to:\n limit the access to critical components; control the access to critical components;  Furthermore, it\u0026rsquo;s important to watch for evidence of tampering.","title":"STRIDE threat modeling on Kubernetes pt.2/6: Tampering"},{"content":"As it comes from the power of the open source and Borg, Kubernetes is an ecosystem very flexible. Only the extensibility of the APIs as for the CRDs opens the world to a vastity of opportunities to build architectures upon it (see the SIG\u0026rsquo;s Cluster API, the AWS EKS and Fargate combinations, etc.).\nAt the same time can be complex to manage, and everyone - or almost everyone - knows that is not enough to get applications working; as part of the administration it is vital to secure your cluster and so your application with your data to get the job done.\nIn security, the threat modeling is the process of identifying vulnerabilities to improve security by preventing the threats introduced by vulnerabilities.\nIn turn, there are different types of threats and the STRIDE model defines 6 categories of them:\n Spoofing Tampering Repudiation Information disclosure Denial of service Elevation of privilege  In this series of short and concise guide for threat prevention on Kubernetes, we\u0026rsquo;ll go through each category of threat starting with the first one.\nSpoofing Spoofing is pretending to be somebody or something you are not, to gain extra privileges. The process that makes sure the presented identity is real is the authentication, and in Kubernetes the authentication is based on mutual TLS.\nWe briefly analyze the situation from the API Server perspective and the Pod perspective.\nAPI Server Starting with the API Server, the mTLS is only as secure as the Certificate Authority, so:\n The CA must be secured, so in particular: the certificates issued by the CA must be used and trusted only within the cluster; outside of Kubernetes the CA should not be trusted. Use two key pairs, one for internal components and one for external components, in particular: use self-signed CA for internal keys; third-party CA(s) for external components' certificates; in this case Kubernetes must be configured to trust it/them.  Pod On the pod perspective, mostly it probably does not need to access the API Server, in which case:\n since pods use Service Account to authenticate to the API server and be authorized by mounting the Service Account token as a Secret, don\u0026rsquo;t mount it on Pods by default. In particular, with Kubernetes 1.6+, specifying it at the Pod level with the automountServiceAccountToken spec:  apiVersion:v1kind:Podmetadata:name:my-podspec:serviceAccountName:build-robotautomountServiceAccountToken:falseYou can configure it also at the Service Account level; keep in mind that the Pod Spec takes precedence over the Service Account if both specify a automountServiceAccountToken value.\nThis is the end of the first pill of this series. Stay tuned for the next part.\nHappy hacking!\n","permalink":"https://blog.maxgio.me/posts/k8s-stride-01-spoofing/","summary":"As it comes from the power of the open source and Borg, Kubernetes is an ecosystem very flexible. Only the extensibility of the APIs as for the CRDs opens the world to a vastity of opportunities to build architectures upon it (see the SIG\u0026rsquo;s Cluster API, the AWS EKS and Fargate combinations, etc.).\nAt the same time can be complex to manage, and everyone - or almost everyone - knows that is not enough to get applications working; as part of the administration it is vital to secure your cluster and so your application with your data to get the job done.","title":"STRIDE threat modeling on Kubernetes pt.1/6: Spoofing"}]