<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>STRIDE threat modeling on Kubernetes pt.5/6: Denial of service | Maxgio's blog</title><meta name=keywords content="kubernetes,security"><meta name=description content="I&rsquo;m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes. In the previous one we talked about Information disclosure. This part is about the D that stands for Denial Of Service.
DOS is the attempt to making a resource unavailable. For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company&rsquo;s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (really happened - thanks Peter)."><meta name=author content><link rel=canonical href=http://maxgio92.github.io/posts/k8s-stride-05-denial-of-service/><link crossorigin=anonymous href=/assets/css/stylesheet.min.149ea7cdaa83f0ab31471ced9e0495af0272de908dd4a38e5c229d8b0579a758.css integrity="sha256-FJ6nzaqD8KsxRxztngSVrwJy3pCN1KOOXCKdiwV5p1g=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=http://maxgio92.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://maxgio92.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://maxgio92.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://maxgio92.github.io/apple-touch-icon.png><link rel=mask-icon href=http://maxgio92.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="STRIDE threat modeling on Kubernetes pt.5/6: Denial of service"><meta property="og:description" content="I&rsquo;m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes. In the previous one we talked about Information disclosure. This part is about the D that stands for Denial Of Service.
DOS is the attempt to making a resource unavailable. For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company&rsquo;s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (really happened - thanks Peter)."><meta property="og:type" content="article"><meta property="og:url" content="http://maxgio92.github.io/posts/k8s-stride-05-denial-of-service/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-09-07T18:36:15+02:00"><meta property="article:modified_time" content="2020-09-07T18:36:15+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="STRIDE threat modeling on Kubernetes pt.5/6: Denial of service"><meta name=twitter:description content="I&rsquo;m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes. In the previous one we talked about Information disclosure. This part is about the D that stands for Denial Of Service.
DOS is the attempt to making a resource unavailable. For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company&rsquo;s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (really happened - thanks Peter)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://maxgio92.github.io/posts/"},{"@type":"ListItem","position":2,"name":"STRIDE threat modeling on Kubernetes pt.5/6: Denial of service","item":"http://maxgio92.github.io/posts/k8s-stride-05-denial-of-service/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"STRIDE threat modeling on Kubernetes pt.5/6: Denial of service","name":"STRIDE threat modeling on Kubernetes pt.5\/6: Denial of service","description":"I\u0026rsquo;m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes. In the previous one we talked about Information disclosure. This part is about the D that stands for Denial Of Service.\nDOS is the attempt to making a resource unavailable. For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company\u0026rsquo;s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (really happened - thanks Peter).","keywords":["kubernetes","security"],"articleBody":"I’m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes. In the previous one we talked about Information disclosure. This part is about the D that stands for Denial Of Service.\nDOS is the attempt to making a resource unavailable. For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company’s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (really happened - thanks Peter).\nTherefore, an induced lack of resources is what generally leads to unavailability.\nSo, how we can do prevention? We can do it with:\n Increased Availability Resource isolation Resource monitoring Moreover, vulnerability-specific patches  Now let’s jump into Kubernetes world and think about splitting up the different layers on which to guarantee availability: Nodes Network Control plane Workload\nAs the availability can be increased on all resources, I’ll sum up briefly what we can do.\nMaster nodes  Deploy multiple master nodes to provide HA on the control plane (for instance to protect from direct attacks to the API server); Deploy on multiple datacenters (to protect from attacks on the network to a particular datacenter).  Worker nodes   Deploy on multiple datacenters (to protect from attacks on the network to a particular datacenter);\n  Configure resource limits per namespace by using ResourceQuotas for:\n CPU and memory; Storage (PVC per StorageClass); Object count; Extended resources (only limit);    Configure resource limits per container;\n can also be useful for scheduling purposes with Pod Priority, and can be able to define the workload’s Quality of Service; use LimitRanges to set resource defaults;    Configure out of resource handling to reclaim resources by notifying under pressure nodes to the kubelet;\n  Configure Cluster Autoscaler to gain availability based on your workload.\n  Workload  Configure Horizontal Pod Autoscaler; Configure correct resources limits other than requests; Configure Vertical Pod Autoscaler or addon-resizer; you can also leverage the VPA in Off mode in order to get only recommendations for setting appropriate resources for your workload; Define Pod-to-Pod and Pod-to-external Network Policies; Configure mutual TLS and proper API authentication mechanism.  API server  Configure high availability; Configure monitoring and alerting on requests and Audit; Isolate: do not expose the endpoint on Internet, for instance syn flood attacks could be in place.  etcd  Configure HA; Configure monitoring and alerting on requests; Isolate: so that only the control plane members can access it; As a plus, configure dedicated cluster, since etcd is one of the main bottlenecks and to provide resilience from the other control plane components (e.g. if they are compromised).  Network  Configure rate limiting at Ingress Controller level to limit connections and requests per seconds/minute per IP (for example with NGINX ingress controller); Deny source IPs with Network policies.  Then, other than following all the best practices there could also be vulnerabilities on components that we generally consider already secured; so let’s sum up a couple of them.\nKnown vulnerabilities CVE-2019–9512: Ping Flood with HTTP/2 The attacker hammers the HTTP/2 listener with a continuous flow of ping requests. To respond, the recipient start queuing the responses, leading to growing queues and then allocating more memory and CPU.\nCVE-2019–9514: Reset Flood with HTTP/2 The attacker can open several streams to the server and sending invalid data through them. Having received invalid data, the server sends HTTP/2 RST_STREAM frames to the attacker to cancel the “invalid” connection.\nWith lots of RST_STREAM responses, they start to queue. As the queue gets more massive, more and more CPU and memory get allocated to the application until it eventually crashes.\nKubernetes has released the required patches to mitigate the issues as mentioned above. The new versions were built using the patched versions of Go so that the required fixed are applied to the net/http library.\nFixed versions:\n Kubernetes v1.15.3 - go1.12.9 Kubernetes v1.14.6 - go1.12. Kubernetes v1.13.10 - go1.11.13  CVE-2020–8557: Node disk DOS The /etc/hosts file mounted in a pod by kubelet is not included by the kubelet eviction manager when calculating ephemeral storage usage by a pod. If a pod writes a large amount of data to the /etc/hosts file, it could fill the storage space of the node. Affected versions:\n kubelet v1.18.0–1.18.5 kubelet v1.17.0–1.17.8 kubelet  Fixed Versions:\n kubelet master - fixed by #92916 kubelet v1.18.6 - fixed by #92921 kubelet v1.17.9 - fixed by #92923 kubelet v1.16.13 - fixed by #92924  Prior to upgrading, this vulnerability can be mitigated by using PodSecurityPolicies or other admission webhooks to force containers to drop CAP_DAC_OVERRIDE or to prohibit privilege escalation and running as root. Consider anyway that these measures may break existing workloads that rely upon these privileges to function properly.\nCVE-2020–8551: Kubelet DoS via API The kubelet has been found to be vulnerable to a denial of service attack via kubelet API, including the unauthenticated HTTP read-only API typically served on port 10255, and the authenticated HTTPS API typically served on port 10250.\nAffected Versions:\n kubelet v1.17.0 - v1.17.2 kubelet v1.16.0 - v1.16.6 kubelet v1.15.0 - v1.15.9  Fixed Versions\n kubelet v1.17.3 kubelet v1.16.7 kubelet v1.15.10  In order to mitigate this issue limit access to the kubelet API or patch the kubelet.\nCVE-2020–8552: Kubernetes API Server OOM The API server has been found to be vulnerable to a denial of service attack via authorized API requests.\nAffected Versions:\n kube-apiserver v1.17.0 - v1.17.2 kube-apiserver v1.16.0 - v1.16.6 kube-apiserver  Fixed Versions:\n kube-apiserver v1.17.3 kube-apiserver v1.16.7 kube-apiserver v1.15.10  Prior to upgrading, this vulnerability can be mitigated by preventing unauthenticated or unauthorized access to all apis and by ensuring that the API server automatically restarts if it OOMs.\nCVE-2019–1002100: Kubernetes API Server JSON-patch parsing Users that are authorized to make patch requests to the Kubernetes API server can send a specially crafted patch of type json-patch (e.g. kubectl patch - type json or Content-Type: application/json-patch+json) that consumes excessive resources while processing, causing a denial of service on the API server.\nAffected versions:\n Kubernetes v1.0.x-1.10.x Kubernetes v1.11.0–1.11.7 Kubernetes v1.12.0–1.12.5 Kubernetes v1.13.0–1.13.3  Fixed Versions:\n Kubernetes v1.11.8 Kubernetes v1.12.6 Kubernetes v1.13.4  Prior to upgrading, this vulnerability can be mitigated by removing patch permissions from untrusted users.\nCVE-2019–11253: Kubernetes API Server JSON/YAML parsing This is a vulnerability in the API server, allowing authorized users sending malicious YAML or JSON payloads to cause kube-apiserver to consume excessive CPU or memory, potentially crashing and becoming unavailable.\nPrior to v1.14.0, default RBAC policy authorized anonymous users to submit requests that could trigger this vulnerability.\nClusters upgraded from a version prior to v1.14.0 keep the more permissive policy by default for backwards compatibility. Here you can find the more restrictive RBAC rules that can mitigate the issue.\nAffected versions:\n Kubernetes v1.0.0–1.12.x Kubernetes v1.13.0–1.13.11 Kubernetes v1.14.0–1.14.7 Kubernetes v1.15.0–1.15.4 Kubernetes v1.16.0–1.16.1  Fixed Versions:\n Kubernetes v1.13.12 Kubernetes v1.14.8 Kubernetes v1.15.5 Kubernetes v1.16.2  Consider that if you are running a version prior to v1.14.0, in addition to installing the restrictive policy, turn off autoupdate for the applied ClusterRoleBinding so your changes aren’t replaced on an API server restart.\nOn the related Github issue you can find more details that I didn’t insert here for conciseness.\nConclusion So, that’s all folks! If we followed all these rules and applied the released patches that’s a good starting point for prevention and can also help on detection and remediation.\nStay tuned for the next and final episode about the E of STRIDE: Escalation of privileges!\n","wordCount":"1220","inLanguage":"en","datePublished":"2020-09-07T18:36:15+02:00","dateModified":"2020-09-07T18:36:15+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://maxgio92.github.io/posts/k8s-stride-05-denial-of-service/"},"publisher":{"@type":"Organization","name":"Maxgio's blog","logo":{"@type":"ImageObject","url":"http://maxgio92.github.io/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href=http://maxgio92.github.io/ accesskey=h title="Maxgio's blog (Alt + H)">Maxgio's blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=http://maxgio92.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://maxgio92.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://maxgio92.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>STRIDE threat modeling on Kubernetes pt.5/6: Denial of service</h1><div class=post-meta><span title="2020-09-07 18:36:15 +0200 +0200">September 7, 2020</span></div></header><div class=post-content><p>I&rsquo;m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes.
In the previous one we talked about Information disclosure. This part is about the D that stands for <strong>Denial Of Service</strong>.</p><p>DOS is the attempt to making a resource unavailable.
For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company&rsquo;s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (<a href=https://redlock.io/blog/cryptojacking-tesla>really happened</a> - thanks <a href=https://dev.to/petermbenjamin>Peter</a>).</p><p>Therefore, an induced lack of resources is what generally leads to unavailability.</p><p>So, how we can do prevention?
We can do it with:</p><ul><li>Increased Availability</li><li>Resource isolation</li><li>Resource monitoring</li><li>Moreover, vulnerability-specific patches</li></ul><p>Now let&rsquo;s jump into Kubernetes world and think about splitting up the different layers on which to guarantee availability:
Nodes
Network
Control plane
Workload</p><p>As the availability can be increased on all resources, I&rsquo;ll sum up briefly what we can do.</p><h1 id=master-nodes>Master nodes<a hidden class=anchor aria-hidden=true href=#master-nodes>#</a></h1><ul><li>Deploy <a href=https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/>multiple master nodes</a> to provide HA on the control plane (for instance to protect from direct attacks to the API server);</li><li>Deploy on multiple datacenters (to protect from attacks on the network to a particular datacenter).</li></ul><h1 id=worker-nodes>Worker nodes<a hidden class=anchor aria-hidden=true href=#worker-nodes>#</a></h1><ul><li><p>Deploy on multiple datacenters (to protect from attacks on the network to a particular datacenter);</p></li><li><p>Configure resource limits per namespace by using <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/><code>ResourceQuotas</code></a> for:</p><ul><li>CPU and memory;</li><li>Storage (<code>PVC</code> per <code>StorageClass</code>);</li><li>Object count;</li><li>Extended resources (only limit);</li></ul></li><li><p>Configure <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>resource limits</a> per container;</p><ul><li>can also be useful for scheduling purposes with <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority</a>, and can be able to define the workload&rsquo;s <a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/>Quality of Service</a>;</li><li>use <a href=https://kubernetes.io/docs/concepts/policy/limit-range/><code>LimitRanges</code></a> to set resource defaults;</li></ul></li><li><p>Configure <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/>out of resource handling</a> to reclaim resources by notifying <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions>under pressure nodes</a> to the <code>kubelet</code>;</p></li><li><p>Configure <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Cluster Autoscaler</a> to gain availability based on your workload.</p></li></ul><h1 id=workload>Workload<a hidden class=anchor aria-hidden=true href=#workload>#</a></h1><ul><li>Configure <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaler</a>;</li><li>Configure correct <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/>resources limits</a> other than requests;</li><li>Configure <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>Vertical Pod Autoscaler</a> or <a href=https://github.com/kubernetes/autoscaler/tree/master/addon-resizer>addon-resizer</a>; you can also leverage the VPA in <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#quick-start><code>Off mode</code></a> in order to get only recommendations for setting appropriate resources for your workload;</li><li>Define Pod-to-Pod and Pod-to-external <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Network Policies</a>;</li><li>Configure mutual TLS and proper API authentication mechanism.</li></ul><h1 id=api-server>API server<a hidden class=anchor aria-hidden=true href=#api-server>#</a></h1><ul><li>Configure <a href=https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/>high availability</a>;</li><li>Configure <a href=https://sysdig.com/blog/monitor-kubernetes-api-server/>monitoring</a> and alerting on requests and <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/audit/><code>Audit</code></a>;</li><li>Isolate: do not expose the endpoint on Internet, for instance <a href=https://en.wikipedia.org/wiki/SYN_flood>syn flood</a> attacks could be in place.</li></ul><h1 id=etcd>etcd<a hidden class=anchor aria-hidden=true href=#etcd>#</a></h1><ul><li>Configure <a href=https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#multi-node-etcd-cluster>HA</a>;</li><li>Configure <a href=https://sysdig.com/blog/monitor-etcd/>monitoring and alerting</a> on requests;</li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#limiting-access-of-etcd-clusters>Isolate</a>: so that only the control plane members can access it;</li><li>As a plus, configure <a href=https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#starting-etcd-clusters>dedicated cluster</a>, since etcd is one of the main bottlenecks and to provide resilience from the other control plane components (e.g. if they are compromised).</li></ul><h1 id=network>Network<a hidden class=anchor aria-hidden=true href=#network>#</a></h1><ul><li>Configure rate limiting at Ingress Controller level to limit connections and requests per seconds/minute per IP (for example <a href=https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#rate-limiting>with NGINX ingress controller</a>);</li><li>Deny source IPs with Network policies.</li></ul><p>Then, other than following all the best practices there could also be vulnerabilities on components that we generally consider already secured; so let&rsquo;s sum up a couple of them.</p><h1 id=known-vulnerabilities>Known vulnerabilities<a hidden class=anchor aria-hidden=true href=#known-vulnerabilities>#</a></h1><h2 id=cve-20199512httpscvemitreorgcgi-bincvenamecginamecve-2019-9512-ping-flood-with-http2><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-9512">CVE-2019–9512</a>: Ping Flood with HTTP/2<a hidden class=anchor aria-hidden=true href=#cve-20199512httpscvemitreorgcgi-bincvenamecginamecve-2019-9512-ping-flood-with-http2>#</a></h2><p>The attacker hammers the HTTP/2 listener with a continuous flow of ping requests. To respond, the recipient start queuing the responses, leading to growing queues and then allocating more memory and CPU.</p><h2 id=cve-20199514httpscvemitreorgcgi-bincvenamecginamecve-2019-9514-reset-flood-with-http2><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-9514">CVE-2019–9514</a>: Reset Flood with HTTP/2<a hidden class=anchor aria-hidden=true href=#cve-20199514httpscvemitreorgcgi-bincvenamecginamecve-2019-9514-reset-flood-with-http2>#</a></h2><p>The attacker can open several streams to the server and sending invalid data through them.
Having received invalid data, the server sends HTTP/2 <code>RST_STREAM</code> frames to the attacker to cancel the &ldquo;invalid&rdquo; connection.</p><p>With lots of <code>RST_STREAM</code> responses, they start to queue.
As the queue gets more massive, more and more CPU and memory get allocated to the application until it eventually crashes.</p><p>Kubernetes has released the required patches to mitigate the issues as mentioned above. The new versions were built using the patched versions of Go so that the required fixed are applied to the net/http library.</p><p>Fixed versions:</p><ul><li>Kubernetes v1.15.3 - go1.12.9</li><li>Kubernetes v1.14.6 - go1.12.</li><li>Kubernetes v1.13.10 - go1.11.13</li></ul><h2 id=cve-20208557httpscvemitreorgcgi-bincvenamecginamecve-2020-8557-node-disk-dos><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8557">CVE-2020–8557</a>: Node disk DOS<a hidden class=anchor aria-hidden=true href=#cve-20208557httpscvemitreorgcgi-bincvenamecginamecve-2020-8557-node-disk-dos>#</a></h2><p>The /etc/hosts file mounted in a pod by kubelet is not included by the kubelet eviction manager when <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#with-imagefs-1>calculating ephemeral storage</a> usage by a pod. If a pod writes a large amount of data to the /etc/hosts file, it could fill the storage space of the node.
Affected versions:</p><ul><li>kubelet v1.18.0–1.18.5</li><li>kubelet v1.17.0–1.17.8</li><li>kubelet &lt; v1.16.13</li></ul><p>Fixed Versions:</p><ul><li>kubelet master - fixed by #92916</li><li>kubelet v1.18.6 - fixed by #92921</li><li>kubelet v1.17.9 - fixed by #92923</li><li>kubelet v1.16.13 - fixed by #92924</li></ul><p>Prior to upgrading, this vulnerability can be mitigated by using PodSecurityPolicies or other admission webhooks to force containers to drop <a href=https://man7.org/linux/man-pages/man7/capabilities.7.html><code>CAP_DAC_OVERRIDE</code></a> or to prohibit privilege escalation and running as root. </p><p>Consider anyway that these measures may break existing workloads that rely upon these privileges to function properly.</p><h2 id=cve-20208551httpscvemitreorgcgi-bincvenamecginamecve-2020-8551-kubelet-dos-via-api><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8551">CVE-2020–8551</a>: Kubelet DoS via API<a hidden class=anchor aria-hidden=true href=#cve-20208551httpscvemitreorgcgi-bincvenamecginamecve-2020-8551-kubelet-dos-via-api>#</a></h2><p>The <code>kubelet</code> has been found to be vulnerable to a denial of service attack via kubelet API, including the unauthenticated HTTP read-only API typically served on port 10255, and the authenticated HTTPS API typically served on port 10250.</p><p>Affected Versions:</p><ul><li>kubelet v1.17.0 - v1.17.2</li><li>kubelet v1.16.0 - v1.16.6</li><li>kubelet v1.15.0 - v1.15.9</li></ul><p>Fixed Versions</p><ul><li>kubelet v1.17.3</li><li>kubelet v1.16.7</li><li>kubelet v1.15.10</li></ul><p>In order to mitigate this issue <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/>limit access to the kubelet API</a> or patch the kubelet.</p><h2 id=cve-20208552httpscvemitreorgcgi-bincvenamecginamecve-2020-8552-kubernetes-api-server-oom><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8552">CVE-2020–8552</a>: Kubernetes API Server OOM<a hidden class=anchor aria-hidden=true href=#cve-20208552httpscvemitreorgcgi-bincvenamecginamecve-2020-8552-kubernetes-api-server-oom>#</a></h2><p>The API server has been found to be vulnerable to a denial of service attack via authorized API requests.</p><p>Affected Versions:</p><ul><li>kube-apiserver v1.17.0 - v1.17.2</li><li>kube-apiserver v1.16.0 - v1.16.6</li><li>kube-apiserver &lt; v1.15.10</li></ul><p>Fixed Versions:</p><ul><li>kube-apiserver v1.17.3</li><li>kube-apiserver v1.16.7</li><li>kube-apiserver v1.15.10</li></ul><p>Prior to upgrading, this vulnerability can be mitigated by <a href=https://kubernetes.io/docs/concepts/security/controlling-access/>preventing unauthenticated or unauthorized access</a> to all apis and by ensuring that the API server automatically restarts if it OOMs.</p><h2 id=cve-20191002100httpscvemitreorgcgi-bincvenamecginamecve-2019-1002100-kubernetes-api-server-json-patch-parsing><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-1002100">CVE-2019–1002100</a>: Kubernetes API Server JSON-patch parsing<a hidden class=anchor aria-hidden=true href=#cve-20191002100httpscvemitreorgcgi-bincvenamecginamecve-2019-1002100-kubernetes-api-server-json-patch-parsing>#</a></h2><p>Users that are authorized to make patch requests to the Kubernetes API server can send a specially crafted patch of type <a href=https://tools.ietf.org/html/rfc6902><code>json-patch</code></a> (e.g. <code>kubectl patch - type json</code> or <code>Content-Type: application/json-patch+json</code>) that consumes excessive resources while processing, causing a denial of service on the API server.</p><p>Affected versions:</p><ul><li>Kubernetes v1.0.x-1.10.x</li><li>Kubernetes v1.11.0–1.11.7</li><li>Kubernetes v1.12.0–1.12.5</li><li>Kubernetes v1.13.0–1.13.3</li></ul><p>Fixed Versions:</p><ul><li>Kubernetes v1.11.8</li><li>Kubernetes v1.12.6</li><li>Kubernetes v1.13.4</li></ul><p>Prior to upgrading, this vulnerability can be mitigated by removing patch permissions from untrusted users.</p><h2 id=cve-201911253httpscvemitreorgcgi-bincvenamecginamecve-2019-11253-kubernetes-api-server-jsonyaml-parsing><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11253">CVE-2019–11253</a>: Kubernetes API Server JSON/YAML parsing<a hidden class=anchor aria-hidden=true href=#cve-201911253httpscvemitreorgcgi-bincvenamecginamecve-2019-11253-kubernetes-api-server-jsonyaml-parsing>#</a></h2><p>This is a vulnerability in the API server, allowing authorized users sending malicious YAML or JSON payloads to cause kube-apiserver to consume excessive CPU or memory, potentially crashing and becoming unavailable.</p><p>Prior to v1.14.0, default RBAC policy authorized anonymous users to submit requests that could trigger this vulnerability.</p><p>Clusters upgraded from a version prior to v1.14.0 keep the more permissive policy by default for backwards compatibility.
Here you can find the more restrictive RBAC rules that can mitigate the issue.</p><p>Affected versions:</p><ul><li>Kubernetes v1.0.0–1.12.x</li><li>Kubernetes v1.13.0–1.13.11</li><li>Kubernetes v1.14.0–1.14.7</li><li>Kubernetes v1.15.0–1.15.4</li><li>Kubernetes v1.16.0–1.16.1</li></ul><p>Fixed Versions:</p><ul><li>Kubernetes v1.13.12</li><li>Kubernetes v1.14.8</li><li>Kubernetes v1.15.5</li><li>Kubernetes v1.16.2</li></ul><p>Consider that if you are running a version prior to v1.14.0, in addition to installing the restrictive policy, turn off autoupdate for the applied ClusterRoleBinding so your changes aren&rsquo;t replaced on an API server restart.</p><p>On the related Github issue you can find more details that I didn&rsquo;t insert here for conciseness.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>So, that&rsquo;s all folks! If we followed all these rules and applied the released patches that&rsquo;s a good starting point for prevention and can also help on detection and remediation.</p><p>Stay tuned for the next and final episode about the E of STRIDE: Escalation of privileges!</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://maxgio92.github.io/tags/kubernetes/>kubernetes</a></li><li><a href=http://maxgio92.github.io/tags/security/>security</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=http://maxgio92.github.io/>Maxgio's blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu')
if(menu){menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>