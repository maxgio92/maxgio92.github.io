<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><style type=text/css>body{font-family:monospace}</style><title>STRIDE threat modeling on Kubernetes pt.5/6: Denial of service</title><link rel=stylesheet href=/css/style.css></head><body><header><h2><a href=http://maxgio92.github.io/>Maxgio's blog</a></h2><br><div style=float:right>Notes that I'd want to share with you</div><br><p><nav><a href=/><b>Home</b></a>.
<a href=/posts/><b>Posts</b></a>.
<a href=/categories/><b>Categories</b></a>.
<a href=/tags/><b>Tags</b></a>.</nav></p></header><main><article><h1>STRIDE threat modeling on Kubernetes pt.5/6: Denial of service</h1><b><time>2020-09-07 18:36:15</time></b>
<a href=/tags/kubernetes>kubernetes</a>
<a href=/tags/security>security</a><div><p>I&rsquo;m back after a long time with the fifth episode of this mini-series about STRIDE threat modeling in Kubernetes.
In the previous one we talked about Information disclosure. This part is about the D that stands for <strong>Denial Of Service</strong>.</p><p>DOS is the attempt to making a resource unavailable.
For instance, a Kubernetes dashboard is left exposed on the Internet, allowing anyone to deploy containers on your company&rsquo;s infrastructure to mine cryptocurrency and starve your legitimate applications of CPU (<a href=https://redlock.io/blog/cryptojacking-tesla>really happened</a> - thanks <a href=https://dev.to/petermbenjamin>Peter</a>).</p><p>Therefore, an induced lack of resources is what generally leads to unavailability.</p><p>So, how we can do prevention?
We can do it with:</p><ul><li>Increased Availability</li><li>Resource isolation</li><li>Resource monitoring</li><li>Moreover, vulnerability-specific patches</li></ul><p>Now let&rsquo;s jump into Kubernetes world and think about splitting up the different layers on which to guarantee availability:
Nodes
Network
Control plane
Workload</p><p>As the availability can be increased on all resources, I&rsquo;ll sum up briefly what we can do.</p><h1 id=master-nodes>Master nodes</h1><ul><li>Deploy <a href=https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/>multiple master nodes</a> to provide HA on the control plane (for instance to protect from direct attacks to the API server);</li><li>Deploy on multiple datacenters (to protect from attacks on the network to a particular datacenter).</li></ul><h1 id=worker-nodes>Worker nodes</h1><ul><li><p>Deploy on multiple datacenters (to protect from attacks on the network to a particular datacenter);</p></li><li><p>Configure resource limits per namespace by using <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/><code>ResourceQuotas</code></a> for:</p><ul><li>CPU and memory;</li><li>Storage (<code>PVC</code> per <code>StorageClass</code>);</li><li>Object count;</li><li>Extended resources (only limit);</li></ul></li><li><p>Configure <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>resource limits</a> per container;</p><ul><li>can also be useful for scheduling purposes with <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority</a>, and can be able to define the workload&rsquo;s <a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/>Quality of Service</a>;</li><li>use <a href=https://kubernetes.io/docs/concepts/policy/limit-range/><code>LimitRanges</code></a> to set resource defaults;</li></ul></li><li><p>Configure <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/>out of resource handling</a> to reclaim resources by notifying <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions>under pressure nodes</a> to the <code>kubelet</code>;</p></li><li><p>Configure <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Cluster Autoscaler</a> to gain availability based on your workload.</p></li></ul><h1 id=workload>Workload</h1><ul><li>Configure <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaler</a>;</li><li>Configure correct <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/>resources limits</a> other than requests;</li><li>Configure <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>Vertical Pod Autoscaler</a> or <a href=https://github.com/kubernetes/autoscaler/tree/master/addon-resizer>addon-resizer</a>; you can also leverage the VPA in <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#quick-start><code>Off mode</code></a> in order to get only recommendations for setting appropriate resources for your workload;</li><li>Define Pod-to-Pod and Pod-to-external <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Network Policies</a>;</li><li>Configure mutual TLS and proper API authentication mechanism.</li></ul><h1 id=api-server>API server</h1><ul><li>Configure <a href=https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/>high availability</a>;</li><li>Configure <a href=https://sysdig.com/blog/monitor-kubernetes-api-server/>monitoring</a> and alerting on requests and <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/audit/><code>Audit</code></a>;</li><li>Isolate: do not expose the endpoint on Internet, for instance <a href=https://en.wikipedia.org/wiki/SYN_flood>syn flood</a> attacks could be in place.</li></ul><h1 id=etcd>etcd</h1><ul><li>Configure <a href=https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#multi-node-etcd-cluster>HA</a>;</li><li>Configure <a href=https://sysdig.com/blog/monitor-etcd/>monitoring and alerting</a> on requests;</li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#limiting-access-of-etcd-clusters>Isolate</a>: so that only the control plane members can access it;</li><li>As a plus, configure <a href=https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#starting-etcd-clusters>dedicated cluster</a>, since etcd is one of the main bottlenecks and to provide resilience from the other control plane components (e.g. if they are compromised).</li></ul><h1 id=network>Network</h1><ul><li>Configure rate limiting at Ingress Controller level to limit connections and requests per seconds/minute per IP (for example <a href=https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#rate-limiting>with NGINX ingress controller</a>);</li><li>Deny source IPs with Network policies.</li></ul><p>Then, other than following all the best practices there could also be vulnerabilities on components that we generally consider already secured; so let&rsquo;s sum up a couple of them.</p><h1 id=known-vulnerabilities>Known vulnerabilities</h1><h2 id=cve-20199512httpscvemitreorgcgi-bincvenamecginamecve-2019-9512-ping-flood-with-http2><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-9512">CVE-2019–9512</a>: Ping Flood with HTTP/2</h2><p>The attacker hammers the HTTP/2 listener with a continuous flow of ping requests. To respond, the recipient start queuing the responses, leading to growing queues and then allocating more memory and CPU.</p><h2 id=cve-20199514httpscvemitreorgcgi-bincvenamecginamecve-2019-9514-reset-flood-with-http2><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-9514">CVE-2019–9514</a>: Reset Flood with HTTP/2</h2><p>The attacker can open several streams to the server and sending invalid data through them.
Having received invalid data, the server sends HTTP/2 <code>RST_STREAM</code> frames to the attacker to cancel the &ldquo;invalid&rdquo; connection.</p><p>With lots of <code>RST_STREAM</code> responses, they start to queue.
As the queue gets more massive, more and more CPU and memory get allocated to the application until it eventually crashes.</p><p>Kubernetes has released the required patches to mitigate the issues as mentioned above. The new versions were built using the patched versions of Go so that the required fixed are applied to the net/http library.</p><p>Fixed versions:</p><ul><li>Kubernetes v1.15.3 - go1.12.9</li><li>Kubernetes v1.14.6 - go1.12.</li><li>Kubernetes v1.13.10 - go1.11.13</li></ul><h2 id=cve-20208557httpscvemitreorgcgi-bincvenamecginamecve-2020-8557-node-disk-dos><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8557">CVE-2020–8557</a>: Node disk DOS</h2><p>The /etc/hosts file mounted in a pod by kubelet is not included by the kubelet eviction manager when <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#with-imagefs-1>calculating ephemeral storage</a> usage by a pod. If a pod writes a large amount of data to the /etc/hosts file, it could fill the storage space of the node.
Affected versions:</p><ul><li>kubelet v1.18.0–1.18.5</li><li>kubelet v1.17.0–1.17.8</li><li>kubelet &lt; v1.16.13</li></ul><p>Fixed Versions:</p><ul><li>kubelet master - fixed by #92916</li><li>kubelet v1.18.6 - fixed by #92921</li><li>kubelet v1.17.9 - fixed by #92923</li><li>kubelet v1.16.13 - fixed by #92924</li></ul><p>Prior to upgrading, this vulnerability can be mitigated by using PodSecurityPolicies or other admission webhooks to force containers to drop <a href=https://man7.org/linux/man-pages/man7/capabilities.7.html><code>CAP_DAC_OVERRIDE</code></a> or to prohibit privilege escalation and running as root. </p><p>Consider anyway that these measures may break existing workloads that rely upon these privileges to function properly.</p><h2 id=cve-20208551httpscvemitreorgcgi-bincvenamecginamecve-2020-8551-kubelet-dos-via-api><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8551">CVE-2020–8551</a>: Kubelet DoS via API</h2><p>The <code>kubelet</code> has been found to be vulnerable to a denial of service attack via kubelet API, including the unauthenticated HTTP read-only API typically served on port 10255, and the authenticated HTTPS API typically served on port 10250.</p><p>Affected Versions:</p><ul><li>kubelet v1.17.0 - v1.17.2</li><li>kubelet v1.16.0 - v1.16.6</li><li>kubelet v1.15.0 - v1.15.9</li></ul><p>Fixed Versions</p><ul><li>kubelet v1.17.3</li><li>kubelet v1.16.7</li><li>kubelet v1.15.10</li></ul><p>In order to mitigate this issue <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/>limit access to the kubelet API</a> or patch the kubelet.</p><h2 id=cve-20208552httpscvemitreorgcgi-bincvenamecginamecve-2020-8552-kubernetes-api-server-oom><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8552">CVE-2020–8552</a>: Kubernetes API Server OOM</h2><p>The API server has been found to be vulnerable to a denial of service attack via authorized API requests.</p><p>Affected Versions:</p><ul><li>kube-apiserver v1.17.0 - v1.17.2</li><li>kube-apiserver v1.16.0 - v1.16.6</li><li>kube-apiserver &lt; v1.15.10</li></ul><p>Fixed Versions:</p><ul><li>kube-apiserver v1.17.3</li><li>kube-apiserver v1.16.7</li><li>kube-apiserver v1.15.10</li></ul><p>Prior to upgrading, this vulnerability can be mitigated by <a href=https://kubernetes.io/docs/concepts/security/controlling-access/>preventing unauthenticated or unauthorized access</a> to all apis and by ensuring that the API server automatically restarts if it OOMs.</p><h2 id=cve-20191002100httpscvemitreorgcgi-bincvenamecginamecve-2019-1002100-kubernetes-api-server-json-patch-parsing><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-1002100">CVE-2019–1002100</a>: Kubernetes API Server JSON-patch parsing</h2><p>Users that are authorized to make patch requests to the Kubernetes API server can send a specially crafted patch of type <a href=https://tools.ietf.org/html/rfc6902><code>json-patch</code></a> (e.g. <code>kubectl patch - type json</code> or <code>Content-Type: application/json-patch+json</code>) that consumes excessive resources while processing, causing a denial of service on the API server.</p><p>Affected versions:</p><ul><li>Kubernetes v1.0.x-1.10.x</li><li>Kubernetes v1.11.0–1.11.7</li><li>Kubernetes v1.12.0–1.12.5</li><li>Kubernetes v1.13.0–1.13.3</li></ul><p>Fixed Versions:</p><ul><li>Kubernetes v1.11.8</li><li>Kubernetes v1.12.6</li><li>Kubernetes v1.13.4</li></ul><p>Prior to upgrading, this vulnerability can be mitigated by removing patch permissions from untrusted users.</p><h2 id=cve-201911253httpscvemitreorgcgi-bincvenamecginamecve-2019-11253-kubernetes-api-server-jsonyaml-parsing><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11253">CVE-2019–11253</a>: Kubernetes API Server JSON/YAML parsing</h2><p>This is a vulnerability in the API server, allowing authorized users sending malicious YAML or JSON payloads to cause kube-apiserver to consume excessive CPU or memory, potentially crashing and becoming unavailable.</p><p>Prior to v1.14.0, default RBAC policy authorized anonymous users to submit requests that could trigger this vulnerability.</p><p>Clusters upgraded from a version prior to v1.14.0 keep the more permissive policy by default for backwards compatibility.
Here you can find the more restrictive RBAC rules that can mitigate the issue.</p><p>Affected versions:</p><ul><li>Kubernetes v1.0.0–1.12.x</li><li>Kubernetes v1.13.0–1.13.11</li><li>Kubernetes v1.14.0–1.14.7</li><li>Kubernetes v1.15.0–1.15.4</li><li>Kubernetes v1.16.0–1.16.1</li></ul><p>Fixed Versions:</p><ul><li>Kubernetes v1.13.12</li><li>Kubernetes v1.14.8</li><li>Kubernetes v1.15.5</li><li>Kubernetes v1.16.2</li></ul><p>Consider that if you are running a version prior to v1.14.0, in addition to installing the restrictive policy, turn off autoupdate for the applied ClusterRoleBinding so your changes aren&rsquo;t replaced on an API server restart.</p><p>On the related Github issue you can find more details that I didn&rsquo;t insert here for conciseness.</p><h1 id=conclusion>Conclusion</h1><p>So, that&rsquo;s all folks! If we followed all these rules and applied the released patches that&rsquo;s a good starting point for prevention and can also help on detection and remediation.</p><p>Stay tuned for the next and final episode about the E of STRIDE: Escalation of privileges!</p></div></article></main><aside><div><div><h3>LATEST POSTS</h3></div><div><ul><li><a href=/posts/stride-threat-modeling-kubernetes-elevation-of-privileges/>STRIDE threat modeling on Kubernetes pt.6/6: Elevation of privileges</a></li><li><a href=/posts/k8s-stride-05-denial-of-service/>STRIDE threat modeling on Kubernetes pt.5/6: Denial of service</a></li><li><a href=/posts/k8s-stride-04-information-disclosure/>STRIDE threat modeling on Kubernetes pt.4/6: Information disclosure</a></li><li><a href=/posts/k8s-stride-03-repudiation/>STRIDE threat modeling on Kubernetes pt.3/6: Repudiation</a></li><li><a href=/posts/k8s-stride-02-tampering/>STRIDE threat modeling on Kubernetes pt.2/6: Tampering</a></li></ul></div></div></aside><footer><p>&copy; 2021 <a href=http://maxgio92.github.io/><b>Maxgio's blog</b></a>.
<a href=https://github.com/maxgio92><b>Github</b></a>.
<a href=https://twitter.com/maxgio92><b>Twitter</b></a>.</p></footer></body></html>